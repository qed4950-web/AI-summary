# retriever.py  (Step3: ê²€ìƒ‰ê¸°)
from __future__ import annotations
import os, sys, json, time, importlib, types, warnings
from pathlib import Path
from dataclasses import dataclass
from typing import List, Dict, Any, Optional

import numpy as np

try:
    import pandas as pd
except Exception:
    pd = None

try:
    import joblib
except Exception:
    joblib = None

try:
    from sklearn.exceptions import InconsistentVersionWarning
except Exception:
    InconsistentVersionWarning = None  # type: ignore


# =========================
# ë ˆê±°ì‹œ ëª¨ë“ˆ ë³„ì¹­ ì£¼ì…
# =========================
def _alias_legacy_modules():
    """
    ê³¼ê±° joblibì´ 'TextCleaner' ëª¨ë“ˆ ê²½ë¡œë¥¼ ê¸°ì–µí•˜ê³  ìˆì„ ë•Œ
    í˜„ì¬ Step2 ëª¨ë“ˆëª…ìœ¼ë¡œ ì—°ê²°í•´ ì¤€ë‹¤.
    """
    candidates = [
        "pipeline",                # í˜„ì¬ Step2 íŒŒì¼ëª… (ì—¬ê¸°ì— ë§ì¶¤)
        "step2_module_progress",   # ì´ì „ ì´ë¦„ë“¤
        "step2_pipeline",
        "Step2_module_progress",
    ]
    for name in candidates:
        try:
            mod = importlib.import_module(name)
            sys.modules["TextCleaner"] = mod
            return True
        except Exception:
            continue
    shim = types.ModuleType("TextCleaner")
    sys.modules["TextCleaner"] = shim
    return False


# =========================
# QueryEncoder
# =========================
class QueryEncoder:
    """Step2 topic_model.joblibì—ì„œ íŒŒì´í”„ë¼ì¸ì„ êº¼ë‚´ ì§ˆì˜/ë¬¸ì„œ ì„ë² ë”© ë³€í™˜"""
    def __init__(self, model_path: Path):
        if joblib is None:
            raise RuntimeError("joblibì´ í•„ìš”í•©ë‹ˆë‹¤. pip install joblib")

        try:
            with warnings.catch_warnings(record=True) as caught:
                warnings.simplefilter("always")
                obj = joblib.load(model_path)
        except ModuleNotFoundError as e:
            if "TextCleaner" in str(e):
                print("âš  ë ˆê±°ì‹œ ëª¨ë¸ ê°ì§€: TextCleaner â†’ pipeline ë³„ì¹­ ì£¼ì… í›„ ì¬ì‹œë„")
                _alias_legacy_modules()
                with warnings.catch_warnings(record=True) as caught:
                    warnings.simplefilter("always")
                    obj = joblib.load(model_path)
            else:
                raise

        if 'caught' in locals():
            for warning in caught:
                if InconsistentVersionWarning and issubclass(warning.category, InconsistentVersionWarning):
                    print("â„¹ï¸ ëª¨ë¸ ë²„ì „ ê²½ê³ : ì €ì¥ëœ ëª¨ë¸ê³¼ í˜„ì¬ scikit-learn ë²„ì „ì´ ë‹¤ë¦…ë‹ˆë‹¤. í•„ìš” ì‹œ 'python infopilot.py train'ìœ¼ë¡œ ì¬í•™ìŠµí•˜ì„¸ìš”.")
                else:
                    warnings.showwarning(warning.message, warning.category, warning.filename, warning.lineno)

        self.pipeline = obj["pipeline"]
        self.tfidf = self.pipeline.named_steps["tfidf"]
        self.svd = self.pipeline.named_steps["svd"]

    def encode_docs(self, texts: List[str]) -> np.ndarray:
        texts = [t if isinstance(t, str) else "" for t in texts]
        X = self.tfidf.transform(texts)
        Z = self.svd.transform(X)
        return Z.astype(np.float32, copy=False)

    def encode_query(self, query: str) -> np.ndarray:
        if not isinstance(query, str):
            query = str(query or "")
        Xq = self.tfidf.transform([query])
        Zq = self.svd.transform(Xq)
        return Zq.astype(np.float32, copy=False)


# =========================
# VectorIndex
# =========================
@dataclass
class IndexPaths:
    emb_npy: Path
    meta_json: Path

class VectorIndex:
    def __init__(self):
        self.Z: Optional[np.ndarray] = None
        self.paths: List[str] = []
        self.exts: List[str] = []
        self.preview: List[str] = []

    @staticmethod
    def _normalize_rows(M: np.ndarray) -> np.ndarray:
        norms = np.linalg.norm(M, axis=1, keepdims=True) + 1e-12
        return (M / norms).astype(np.float32, copy=False)

    def build(self, embeddings: np.ndarray, paths: List[str], exts: List[str], texts: List[str]):
        if embeddings.ndim != 2:
            raise ValueError("embeddingsëŠ” 2ì°¨ì›ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        self.Z = self._normalize_rows(embeddings)
        self.paths = list(paths)
        self.exts = list(exts)
        self.preview = [(t[:180] + "â€¦") if len(t) > 180 else t for t in texts]

    def save(self, out_dir: Path) -> IndexPaths:
        out_dir.mkdir(parents=True, exist_ok=True)
        emb_path = out_dir / "doc_embeddings.npy"
        meta_path = out_dir / "doc_meta.json"
        if self.Z is None:
            raise RuntimeError("ì¸ë±ìŠ¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. build() í›„ ì €ì¥í•˜ì„¸ìš”.")
        np.save(emb_path, self.Z)
        with meta_path.open("w", encoding="utf-8") as f:
            json.dump({"paths": self.paths, "exts": self.exts, "preview": self.preview}, f, ensure_ascii=False)
        return IndexPaths(emb_npy=emb_path, meta_json=meta_path)

    def load(self, emb_npy: Path, meta_json: Path):
        self.Z = np.load(emb_npy).astype(np.float32, copy=False)
        with meta_json.open("r", encoding="utf-8") as f:
            meta = json.load(f)
        self.paths = meta["paths"]
        self.exts = meta["exts"]
        self.preview = meta["preview"]
        if self.Z.shape[0] != len(self.paths):
            raise RuntimeError("ì„ë² ë”© í–‰ ìˆ˜ì™€ ë©”íƒ€ í•­ëª© ìˆ˜ê°€ ë‹¤ë¦…ë‹ˆë‹¤.")

    def search(self, qvec: np.ndarray, top_k: int = 5) -> List[Dict[str, Any]]:
        if self.Z is None:
            raise RuntimeError("ì¸ë±ìŠ¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        qv = qvec.reshape(1, -1)
        qv = qv / (np.linalg.norm(qv, axis=1, keepdims=True) + 1e-12)
        sims = (self.Z @ qv.T).ravel()
        idx = np.argpartition(-sims, kth=min(top_k, len(sims)-1))[:top_k]
        idx = idx[np.argsort(-sims[idx])]
        return [
            {"path": self.paths[i], "ext": self.exts[i], "similarity": float(sims[i]), "preview": self.preview[i]}
            for i in idx
        ]


# =========================
# Retriever
# =========================
class Retriever:
    def __init__(self, model_path: Path, corpus_path: Path, cache_dir: Path = Path("./index_cache")):
        self.model_path = Path(model_path)
        self.corpus_path = Path(corpus_path)
        self.cache_dir = Path(cache_dir)
        self.encoder = QueryEncoder(self.model_path)
        self.index = VectorIndex()
        self._ready = False

    def ready(self, rebuild: bool = False):
        emb_npy = self.cache_dir / "doc_embeddings.npy"
        meta_json = self.cache_dir / "doc_meta.json"
        if not rebuild and emb_npy.exists() and meta_json.exists():
            self.index.load(emb_npy, meta_json)
            self._ready = True
            print(f"âœ… ì¸ë±ìŠ¤ ë¡œë“œ: {self.cache_dir}")
            return

        if pd is None:
            raise RuntimeError("pandas í•„ìš”. pip install pandas")

        print("ğŸ“¥ ì½”í¼ìŠ¤ ë¡œë“œâ€¦")
        if self.corpus_path.suffix.lower() == ".parquet":
            try:
                df = pd.read_parquet(self.corpus_path)
            except Exception:
                df = pd.read_csv(
                    self.corpus_path.with_suffix(".csv"),
                    dtype=str,
                    na_filter=False,
                    encoding="utf-8-sig",
                )
        else:
            df = pd.read_csv(
                self.corpus_path,
                dtype=str,
                na_filter=False,
                encoding="utf-8-sig",
            )

        if "text" in df.columns:
            df["text"] = df["text"].fillna("").astype(str)
        if "content" in df.columns:
            df["content"] = df["content"].fillna("").astype(str)
        if "path" in df.columns:
            df["path"] = df["path"].fillna("").astype(str)
        if "ext" in df.columns:
            df["ext"] = df["ext"].fillna("").astype(str)

        mask = df["text"].astype(str).str.len() > 0
        work = df[mask].copy()
        if len(work) == 0:
            raise RuntimeError("ìœ íš¨ í…ìŠ¤íŠ¸ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

        print(f"ğŸ§  ë¬¸ì„œ ì„ë² ë”© ìƒì„±â€¦ (docs={len(work):,})")
        Z = self.encoder.encode_docs(work["text"].tolist())
        if "content" in work.columns:
            preview_source = work["content"].fillna("").astype(str).tolist()
        else:
            preview_source = work["text"].fillna("").astype(str).tolist()
        text_fallback = work["text"].fillna("").astype(str).tolist()
        previews = []
        for preview, fallback in zip(preview_source, text_fallback):
            previews.append(preview or fallback)
        self.index.build(Z, work["path"].tolist(), work["ext"].tolist(), previews)
        paths = self.index.save(self.cache_dir)
        print(f"ğŸ’¾ ì¸ë±ìŠ¤ ì €ì¥: {paths.emb_npy}, {paths.meta_json}")
        self._ready = True

    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        if not self._ready:
            self.ready(False)
        q = self.encoder.encode_query(query)
        return self.index.search(q, top_k)

    @staticmethod
    def format_results(query: str, results: List[Dict[str, Any]]) -> str:
        if not results:
            return f"â€œ{query}â€ì™€ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        lines = [f"â€˜{query}â€™ì™€ ìœ ì‚¬í•œ ë¬¸ì„œ Top {len(results)}:"]
        for i, r in enumerate(results, 1):
            lines.append(f"{i}. {r['path']} [{r['ext']}]  ìœ ì‚¬ë„={r['similarity']:.2f}")
            if r.get("preview"):
                lines.append(f"   ë¯¸ë¦¬ë³´ê¸°: {r['preview']}")
        return "\n".join(lines)
