# infopilot.py
from __future__ import annotations

import argparse
import math
import queue
import threading
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

import csv
import csv
csv.field_size_limit(10**7)  # 10MBê¹Œì§€ í—ˆìš©

import numpy as np

try:
    import pandas as pd
except Exception:
    pd = None

try:
    from sentence_transformers import SentenceTransformer
except Exception:
    SentenceTransformer = None

try:
    import joblib
except Exception:
    joblib = None

try:
    from watchdog.events import FileSystemEventHandler
    from watchdog.observers import Observer
except Exception:
    FileSystemEventHandler = object  # type: ignore
    Observer = None


# ëª¨ë“ˆ ì„í¬íŠ¸
from filefinder import FileFinder
from pipeline import (
    run_step2,
    TrainConfig,
    DEFAULT_N_COMPONENTS,
    DEFAULT_EMBED_MODEL,
    update_corpus_file,
    remove_from_corpus,
    CorpusBuilder,
)
from lnp_chat import LNPChat # ìƒˆë¡œìš´ LNP Chat í´ë˜ìŠ¤ë¥¼ ì„í¬íŠ¸
from retriever import VectorIndex, MODEL_TEXT_COLUMN, _split_tokens


NORMALIZED_ALIASES = {
    "path": ("path", "filepath", "file_path", "fullpath", "full_path", "absolute_path"),
    "size": ("size", "filesize", "file_size", "bytes"),
    "mtime": ("mtime", "modified", "modified_time", "lastmodified", "timestamp"),
    "ctime": ("ctime", "created", "created_time", "creation", "creation_time"),
    "ext": ("ext", "extension", "suffix"),
    "drive": ("drive", "volume", "root"),
    "owner": ("owner", "user", "username", "author", "created_by"),
}


def _normalize_key(name: str) -> str:
    """Normalize header names by stripping non-alphanumerics and lowering case."""
    return "".join(ch for ch in (name or "").lower() if ch.isalnum())


def _pick_value(row: Dict[str, str], aliases) -> str:
    normalized = {_normalize_key(k): (k, v) for k, v in row.items() if k}
    for alias in aliases:
        alias_norm = _normalize_key(alias)
        data = normalized.get(alias_norm)
        if data:
            value = (data[1] or "").strip()
            if value:
                return value
    return ""


def _normalize_scan_row(raw: Dict[str, str], *, context: str = "") -> Dict[str, Any] | None:
    path = _pick_value(raw, NORMALIZED_ALIASES["path"])
    if not path:
        columns = ", ".join(k for k in raw.keys() if k)
        location = f" ({context})" if context else ""
        print(f"âš ï¸ ê²½ê³ : 'path' ê°’ì„ ì°¾ì§€ ëª»í•´ í–‰ì„ ê±´ë„ˆëœë‹ˆë‹¤{location}. (ê°ì§€í•œ ì—´: {columns or 'ì—†ìŒ'})")
        return None

    size_raw = _pick_value(raw, NORMALIZED_ALIASES["size"])
    mtime_raw = _pick_value(raw, NORMALIZED_ALIASES["mtime"])
    ext = _pick_value(raw, NORMALIZED_ALIASES["ext"])
    drive = _pick_value(raw, NORMALIZED_ALIASES["drive"])
    ctime_raw = _pick_value(raw, NORMALIZED_ALIASES["ctime"])
    owner = _pick_value(raw, NORMALIZED_ALIASES["owner"])

    def to_int(value: str) -> int:
        try:
            return int(float(value))
        except (TypeError, ValueError):
            return 0

    def to_float(value: str) -> float:
        try:
            out = float(value)
            if math.isnan(out) or math.isinf(out):
                return 0.0
            return out
        except (TypeError, ValueError):
            return 0.0

    normalized = dict(raw)
    normalized["path"] = path
    normalized["size"] = to_int(size_raw)
    normalized["mtime"] = to_float(mtime_raw)
    normalized["ctime"] = to_float(ctime_raw)
    if ext:
        normalized["ext"] = ext
    if drive:
        normalized["drive"] = drive
    if owner:
        normalized["owner"] = owner
    return normalized


def _parse_roots(raw_roots: List[str] | None) -> List[Path] | None:
    if not raw_roots:
        return None
    roots: List[Path] = []
    for raw in raw_roots:
        p = Path(raw).expanduser().resolve()
        if not p.exists():
            print(f"âš ï¸ ê²½ê³ : ì§€ì •í•œ ë£¨íŠ¸ '{p}'ì´(ê°€) ì¡´ì¬í•˜ì§€ ì•Šì•„ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
        roots.append(p)
    if not roots:
        print("âš ï¸ ê²½ê³ : ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë£¨íŠ¸ê°€ ì—†ì–´ ê¸°ë³¸ ì „ì²´ ìŠ¤ìº”ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
        return None
    return roots


def _run_scan(out: Path, roots: List[Path] | None = None) -> List[Dict[str, Any]]:
    finder = FileFinder(
        exts=FileFinder.DEFAULT_EXTS,
        scan_all_drives=True,
        start_from_current_drive_only=False,
        follow_symlinks=False,
        max_depth=None,
        show_progress=True,
        progress_update_secs=0.5,
        estimate_total_dirs=False,
        startup_banner=True,
    )
    files = finder.find(roots=roots, run_async=False)
    FileFinder.to_csv(files, out)
    print(f"ğŸ“¦ ìŠ¤ìº” ê²°ê³¼ ì €ì¥: {out}")
    return files


def cmd_scan(args):
    roots = _parse_roots(args.roots)
    _run_scan(Path(args.out), roots)


def _resolve_scan_csv(path: Path) -> Path:
    if path.exists():
        return path

    search_root = path.parent if path.parent else Path(".")
    candidates = []
    for candidate in sorted(search_root.glob("*.csv"), key=lambda p: p.stat().st_mtime, reverse=True):
        try:
            with candidate.open("r", encoding="utf-8", newline="") as f:
                reader = csv.DictReader(f)
                headers = reader.fieldnames or []
        except OSError:
            continue
        header_norm = {_normalize_key(h) for h in headers}
        if any(_normalize_key(alias) in header_norm for alias in NORMALIZED_ALIASES["path"]):
            candidates.append(candidate)

    if candidates:
        picked = candidates[0]
        print(f"âš ï¸ '{path}' íŒŒì¼ì´ ì—†ì–´ '{picked}'ì„(ë¥¼) ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return picked

    raise FileNotFoundError(f"ìŠ¤ìº” CSVë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")


def _load_scan_rows(scan_csv: Path) -> List[Dict[str, Any]]:
    rows: List[Dict[str, Any]] = []
    with scan_csv.open("r", encoding="utf-8", newline="") as f:
        reader = csv.DictReader(f)
        for idx, raw in enumerate(reader, start=2):
            normalized = _normalize_scan_row(raw, context=f"{scan_csv}:{idx}")
            if normalized:
                rows.append(normalized)
    return rows


def _sync_scan_csv(
    scan_csv: Path,
    rows_to_add: List[Dict[str, Any]],
    paths_to_remove: Set[str],
) -> None:
    fieldnames = ["path", "size", "mtime", "ctime", "ext", "drive", "owner"]
    existing: Dict[str, Dict[str, Any]] = {}

    if scan_csv.exists():
        with scan_csv.open("r", encoding="utf-8", newline="") as f:
            reader = csv.DictReader(f)
            for row in reader:
                path_key = str(row.get("path", "")).strip()
                if not path_key:
                    continue
                existing[path_key] = {k: row.get(k) for k in fieldnames}

    for raw_path in paths_to_remove:
        key = str(raw_path).strip()
        existing.pop(key, None)

    for row in rows_to_add:
        key = str(row.get("path", "")).strip()
        if not key:
            continue
        normalized_row = {k: row.get(k) for k in fieldnames}
        existing[key] = {**existing.get(key, {}), **normalized_row}

    scan_csv.parent.mkdir(parents=True, exist_ok=True)
    with scan_csv.open("w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for record in existing.values():
            writer.writerow(record)


def _load_sentence_encoder(model_path: Path) -> Tuple[Optional[SentenceTransformer], int, str]:
    model_name = DEFAULT_EMBED_MODEL
    batch_size = 32

    if joblib is not None and model_path.exists():
        try:
            payload = joblib.load(model_path)
            model_name = payload.get("model_name", model_name)
            cfg = payload.get("train_config")
            if cfg and hasattr(cfg, "embedding_batch_size"):
                batch_size = int(getattr(cfg, "embedding_batch_size", batch_size) or batch_size)
        except Exception as exc:
            print(f"âš ï¸ ì„ë² ë”© ëª¨ë¸ ë©”íƒ€ ë¡œë“œ ì‹¤íŒ¨ â†’ ê¸°ë³¸ê°’ ì‚¬ìš©({model_name}): {exc}")

    if SentenceTransformer is None:
        print("âš ï¸ 'sentence-transformers' íŒ¨í‚¤ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì„ë² ë”©ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None, batch_size, model_name

    try:
        encoder = SentenceTransformer(model_name)
    except Exception as exc:
        print(f"âš ï¸ SentenceTransformer ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨({model_name}): {exc}")
        return None, batch_size, model_name
    return encoder, batch_size, model_name


def _load_vector_index(cache_dir: Path) -> VectorIndex:
    cache_dir.mkdir(parents=True, exist_ok=True)
    meta = cache_dir / "doc_meta.json"
    emb = cache_dir / "doc_embeddings.npy"
    faiss_path = cache_dir / "doc_index.faiss"

    index = VectorIndex()
    if meta.exists():
        try:
            index.load(
                emb if emb.exists() else None,
                meta,
                faiss_path=faiss_path if faiss_path.exists() else None,
                use_mmap=False,
            )
        except Exception as exc:
            print(f"âš ï¸ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨ â†’ ìƒˆ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤: {exc}")
            index = VectorIndex()
    return index


class WatchEventHandler(FileSystemEventHandler):
    def __init__(self, event_queue: "queue.Queue[Tuple[str, str]]", allowed_exts: Set[str]) -> None:
        super().__init__()
        self._queue = event_queue
        self._allowed_exts = {ext.lower() for ext in allowed_exts}

    def _should_process(self, path: str) -> bool:
        if not path:
            return False
        ext = Path(path).suffix.lower()
        return ext in self._allowed_exts

    def on_created(self, event):  # type: ignore[override]
        if getattr(event, "is_directory", False):
            return
        if self._should_process(event.src_path):
            self._queue.put(("created", event.src_path))

    def on_modified(self, event):  # type: ignore[override]
        if getattr(event, "is_directory", False):
            return
        if self._should_process(event.src_path):
            self._queue.put(("modified", event.src_path))

    def on_moved(self, event):  # type: ignore[override]
        if getattr(event, "is_directory", False):
            return
        if self._should_process(event.src_path):
            self._queue.put(("deleted", event.src_path))
        if self._should_process(event.dest_path):
            self._queue.put(("created", event.dest_path))

    def on_deleted(self, event):  # type: ignore[override]
        if getattr(event, "is_directory", False):
            return
        if self._should_process(event.src_path):
            self._queue.put(("deleted", event.src_path))


class IncrementalPipeline:
    def __init__(
        self,
        *,
        encoder: SentenceTransformer,
        batch_size: int,
        scan_csv: Path,
        corpus_path: Path,
        cache_dir: Path,
        translate: bool,
    ) -> None:
        self.encoder = encoder
        self.batch_size = max(1, int(batch_size))
        self.scan_csv = scan_csv
        self.corpus_path = corpus_path
        self.cache_dir = cache_dir
        self.translate = translate
        self.allowed_exts = {ext.lower() for ext in FileFinder.DEFAULT_EXTS}
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def process(self, add_paths: Set[str], remove_paths: Set[str]) -> None:
        if pd is None:
            raise RuntimeError("pandas í•„ìš”. pip install pandas")
        add_paths = {p for p in add_paths if Path(p).suffix.lower() in self.allowed_exts}
        remove_paths = {p for p in remove_paths if Path(p).suffix.lower() in self.allowed_exts}

        rows_to_add: List[Dict[str, Any]] = []
        for raw_path in sorted(add_paths):
            meta = FileFinder.collect_file_metadata(Path(raw_path), allowed_exts=self.allowed_exts)
            if meta:
                rows_to_add.append(meta)

        _sync_scan_csv(self.scan_csv, rows_to_add, {str(p) for p in remove_paths})

        if remove_paths:
            remove_from_corpus(list(remove_paths), self.corpus_path)

        new_records = None
        if rows_to_add:
            cb = CorpusBuilder(progress=False, translate=self.translate)
            new_records = cb.build(rows_to_add)
        else:
            new_records = None

        if new_records is not None and not new_records.empty:
            update_corpus_file(new_records, self.corpus_path)

        index = _load_vector_index(self.cache_dir)

        paths_to_remove = set(remove_paths)
        paths_to_remove.update(row["path"] for row in rows_to_add if "path" in row)
        if paths_to_remove:
            index.remove_paths(paths_to_remove)

        if new_records is None or new_records.empty:
            index.save(self.cache_dir)
            if rows_to_add or remove_paths:
                print(
                    f"âš¡ watcher: removed {len(paths_to_remove)} ë¬¸ì„œ, ìƒˆ ë¬¸ì„œ ì—†ìŒ.",
                    flush=True,
                )
            return

        valid_mask = new_records.get("ok", True)
        if pd is not None and isinstance(valid_mask, pd.Series):
            valid_df = new_records[valid_mask & (new_records[MODEL_TEXT_COLUMN].astype(str).str.len() > 0)].copy()
        else:
            valid_df = new_records.copy()

        if valid_df.empty:
            index.save(self.cache_dir)
            print(
                f"âš¡ watcher: ê°±ì‹  {len(rows_to_add)}ê±´ ì¤‘ ìœ íš¨ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.",
                flush=True,
            )
            return

        texts = valid_df[MODEL_TEXT_COLUMN].astype(str).tolist()
        embeddings = self.encoder.encode(
            texts,
            batch_size=self.batch_size,
            show_progress_bar=False,
            convert_to_numpy=True,
            normalize_embeddings=False,
        )
        embeddings = np.asarray(embeddings, dtype=np.float32)

        token_lists = [[tok for tok in _split_tokens(text.lower()) if tok] for text in texts]
        previews_series = valid_df["text_original"] if "text_original" in valid_df.columns else valid_df["text"]
        previews = previews_series.fillna("").astype(str).tolist()

        for idx, (_, row) in enumerate(valid_df.iterrows()):
            index.upsert(
                path=str(row.get("path", "")),
                ext=str(row.get("ext", "")),
                embedding=embeddings[idx],
                preview=previews[idx],
                size=int(row.get("size", 0) or 0),
                mtime=float(row.get("mtime", 0.0) or 0.0),
                ctime=float(row.get("ctime", 0.0) or 0.0),
                owner=str(row.get("owner", "") or ""),
                tokens=token_lists[idx],
            )

        index.save(self.cache_dir)
        print(
            f"âš¡ watcher: ë¬¸ì„œ {len(valid_df)}ê±´ ì—…ë°ì´íŠ¸ (ì œê±° {len(paths_to_remove)})",
            flush=True,
        )


def _watch_loop(
    event_queue: "queue.Queue[Tuple[str, str]]",
    pipeline_ctx: IncrementalPipeline,
    stop_event: threading.Event,
    debounce_sec: float,
) -> None:
    pending_add: Set[str] = set()
    pending_remove: Set[str] = set()
    last_event = 0.0

    while not stop_event.is_set():
        try:
            event_type, path = event_queue.get(timeout=0.5)
            path = str(path)
            if event_type == "deleted":
                pending_remove.add(path)
                pending_add.discard(path)
            else:
                pending_add.add(path)
                pending_remove.discard(path)
            last_event = time.time()
        except queue.Empty:
            pass

        now = time.time()
        if (pending_add or pending_remove) and (now - last_event) >= debounce_sec:
            to_add = set(pending_add)
            to_remove = set(pending_remove)
            pending_add.clear()
            pending_remove.clear()
            try:
                pipeline_ctx.process(to_add, to_remove)
            except Exception as exc:
                print(f"âš ï¸ ì¦ë¶„ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {exc}")

    # Flush remaining events
    if pending_add or pending_remove:
        try:
            pipeline_ctx.process(set(pending_add), set(pending_remove))
        except Exception as exc:
            print(f"âš ï¸ ì¦ë¶„ íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {exc}")


def _build_train_config(args) -> TrainConfig:
    return TrainConfig(
        max_features=args.max_features,
        n_components=args.n_components,
        n_clusters=args.n_clusters,
        ngram_range=(1, 2),
        min_df=args.min_df,
        max_df=args.max_df,
        use_sentence_transformer=getattr(args, "use_embedding", True),
        embedding_model=getattr(args, "embedding_model", DEFAULT_EMBED_MODEL),
        embedding_batch_size=getattr(args, "embedding_batch_size", 32),
    )


def _maybe_limit_rows(rows: List[Dict[str, Any]], limit: int) -> List[Dict[str, Any]]:
    if limit and len(rows) > limit:
        print(f"âš¡ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ìƒìœ„ {limit}ê°œ íŒŒì¼ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return rows[:limit]
    return rows


def _default_train_config() -> TrainConfig:
    return TrainConfig(
        max_features=50000,
        n_components=DEFAULT_N_COMPONENTS,
        n_clusters=25,
        ngram_range=(1, 2),
        min_df=2,
        max_df=0.85,
        use_sentence_transformer=True,
        embedding_model=DEFAULT_EMBED_MODEL,
        embedding_batch_size=32,
    )


def _ensure_chat_artifacts(
    scan_csv: Path,
    corpus: Path,
    model: Path,
    *,
    translate: bool,
    auto_train: bool,
) -> bool:
    """Ensure chat artifacts exist and are up to date. Returns True if training ran."""

    def mtime(path: Path) -> float:
        try:
            return path.stat().st_mtime
        except OSError:
            return 0.0

    resolved_scan: Optional[Path] = None
    if scan_csv:
        try:
            resolved_scan = _resolve_scan_csv(scan_csv)
        except FileNotFoundError:
            resolved_scan = None

    artifacts_exist = corpus.exists() and model.exists()
    needs_train = not artifacts_exist

    if not needs_train and resolved_scan:
        scan_mtime = mtime(resolved_scan)
        artifacts_mtime = min(mtime(corpus), mtime(model))
        if scan_mtime > artifacts_mtime:
            needs_train = True

    if not needs_train:
        print("ğŸ”„ ì¸ë±ìŠ¤ ìµœì‹ ì„± í™•ì¸ ì™„ë£Œ.")
        return False

    if resolved_scan is None:
        msg = (
            "âš ï¸ í•™ìŠµ ì‚°ì¶œë¬¼ì´ ì—†ê±°ë‚˜ ì˜¤ë˜ë˜ì—ˆì§€ë§Œ ì‚¬ìš©í•  ìŠ¤ìº” CSVë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            " `--scan_csv` ê²½ë¡œë¥¼ í™•ì¸í•˜ê±°ë‚˜ 'scan' ëª…ë ¹ì„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
        )
        raise FileNotFoundError(msg)

    if not auto_train:
        raise RuntimeError(
            "í•™ìŠµ ì‚°ì¶œë¬¼ì´ ìµœì‹ ì´ ì•„ë‹™ë‹ˆë‹¤. 'python infopilot.py train --scan_csv "
            f"{resolved_scan}'ë¥¼ ì‹¤í–‰í•œ ë’¤ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        )

    print("âš ï¸ ìŠ¤ìº” ê²°ê³¼ê°€ ëª¨ë¸ë³´ë‹¤ ìµœì‹ ì…ë‹ˆë‹¤. ìë™ìœ¼ë¡œ train ë‹¨ê³„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    rows = _load_scan_rows(resolved_scan)
    if not rows:
        raise ValueError("ìë™ í•™ìŠµì„ ìœ„í•œ ìœ íš¨í•œ í–‰ì´ ì—†ìŠµë‹ˆë‹¤. ìŠ¤ìº” ê²°ê³¼ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    cfg = _default_train_config()
    run_step2(
        rows,
        out_corpus=corpus,
        out_model=model,
        cfg=cfg,
        use_tqdm=True,
        translate=translate,
    )
    print("âœ… ìë™ í•™ìŠµ ì™„ë£Œ")
    return True


def cmd_train(args):
    scan_csv = _resolve_scan_csv(Path(args.scan_csv))
    rows = _load_scan_rows(scan_csv)
    rows = _maybe_limit_rows(rows, args.limit_files)

    if not rows:
        raise ValueError("ìœ íš¨í•œ í•™ìŠµ ëŒ€ìƒ í–‰ì´ ì—†ìŠµë‹ˆë‹¤. ìŠ¤ìº” CSVë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    cfg = _build_train_config(args)
    out_corpus = Path(args.corpus)
    out_model = Path(args.model)
    df, tm = run_step2(rows, out_corpus=out_corpus, out_model=out_model, cfg=cfg, use_tqdm=True, translate=args.translate)
    print("âœ… í•™ìŠµ ì™„ë£Œ")


def cmd_pipeline(args):
    out = Path(args.out)
    roots = _parse_roots(args.roots)
    _run_scan(out, roots)
    rows = _load_scan_rows(out)
    rows = _maybe_limit_rows(rows, args.limit_files)

    if not rows:
        raise ValueError("ìœ íš¨í•œ í•™ìŠµ ëŒ€ìƒ í–‰ì´ ì—†ìŠµë‹ˆë‹¤. ìŠ¤ìº” ê²°ê³¼ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    cfg = _build_train_config(args)
    out_corpus = Path(args.corpus)
    out_model = Path(args.model)
    df, tm = run_step2(rows, out_corpus=out_corpus, out_model=out_model, cfg=cfg, use_tqdm=True, translate=args.translate)
    print("âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")


def cmd_chat(args):
    """ëŒ€í™”í˜• ê²€ìƒ‰ ëª¨ë“œ (LNPChat ì‚¬ìš©)"""
    auto_trained = _ensure_chat_artifacts(
        scan_csv=Path(args.scan_csv),
        corpus=Path(args.corpus),
        model=Path(args.model),
        translate=args.translate,
        auto_train=args.auto_train,
    )

    # LNPChat í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ì¤€ë¹„
    chat_session = LNPChat(
        model_path=Path(args.model),
        corpus_path=Path(args.corpus),
        cache_dir=Path(args.cache),
        topk=args.topk,
        translate=args.translate, # ë²ˆì—­ ì˜µì…˜ ì „ë‹¬
        rerank=args.rerank,
        rerank_model=args.rerank_model,
        rerank_depth=args.rerank_depth,
        rerank_batch_size=args.rerank_batch_size,
        rerank_device=args.rerank_device or None,
        rerank_min_score=args.rerank_min_score,
        lexical_weight=args.lexical_weight,
        show_translation=args.show_translation,
        translation_lang=args.translation_lang,
        min_similarity=args.min_similarity,
    )
    chat_session.ready(rebuild=auto_trained)

    print("\nğŸ’¬ InfoPilot Chat ëª¨ë“œì…ë‹ˆë‹¤. (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'ì¢…ë£Œ' ì…ë ¥)")
    while True:
        try:
            query = input("ì§ˆë¬¸> ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        if not query:
            continue
        if query.lower() in {"exit", "quit", "ì¢…ë£Œ"}:
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        # LNPChatì˜ ask ë©”ì†Œë“œ í˜¸ì¶œ
        result = chat_session.ask(query)
        print(result["answer"])
        if result.get("suggestions"):
            print("\nğŸ’¡ ì´ëŸ° ì§ˆë¬¸ì€ ì–´ë– ì„¸ìš”?")
            for s in result["suggestions"]:
                print(f"   - {s}")
        print("-" * 80)


def cmd_watch(args):
    if Observer is None:
        raise RuntimeError("watchdog ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install watchdog")

    encoder, batch_size, model_name = _load_sentence_encoder(Path(args.model))
    if encoder is None:
        raise RuntimeError("sentence-transformers ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ì–´ watcherë¥¼ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    roots = _parse_roots(args.roots)
    if not roots:
        roots = [Path.cwd()]

    event_queue: "queue.Queue[Tuple[str, str]]" = queue.Queue()
    allowed_exts = {ext.lower() for ext in FileFinder.DEFAULT_EXTS}
    handler = WatchEventHandler(event_queue, allowed_exts)
    observer = Observer()
    for root in roots:
        observer.schedule(handler, str(root), recursive=True)

    pipeline_ctx = IncrementalPipeline(
        encoder=encoder,
        batch_size=batch_size,
        scan_csv=Path(args.scan_csv),
        corpus_path=Path(args.corpus),
        cache_dir=Path(args.cache),
        translate=args.translate,
    )

    debounce_sec = max(0.5, args.debounce_ms / 1000.0)
    stop_event = threading.Event()

    print(
        "ğŸ‘€ íŒŒì¼ ë³€ê²½ ê°ì‹œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. (Ctrl+Cë¡œ ì¢…ë£Œ)"
        f"\n   roots: {', '.join(str(r) for r in roots)}"
        f"\n   embedding model: {model_name} (batch={batch_size})"
        f"\n   debounce: {debounce_sec:.2f}s"
    )

    observer.start()
    try:
        _watch_loop(event_queue, pipeline_ctx, stop_event, debounce_sec)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ watcherë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    finally:
        stop_event.set()
        observer.stop()
        observer.join()


def main():
    ap = argparse.ArgumentParser(prog="infopilot", description="InfoPilot CLI - ë‹¤êµ­ì–´ ë¬¸ì„œ ê²€ìƒ‰ê¸°")
    sp = ap.add_subparsers(dest="cmd", required=True)

    # scan
    ap_scan = sp.add_parser("scan", help="ë“œë¼ì´ë¸Œ ìŠ¤ìº”í•˜ì—¬ íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘")
    ap_scan.add_argument("--out", default="./data/found_files.csv")
    ap_scan.add_argument(
        "--root",
        "--roots",
        dest="roots",
        action="append",
        help="ìŠ¤ìº”í•  ë£¨íŠ¸ ë””ë ‰í„°ë¦¬. ì—¬ëŸ¬ ë²ˆ ì§€ì • ê°€ëŠ¥. ë¯¸ì§€ì • ì‹œ ì „ì²´ ìŠ¤ìº”.",
    )
    ap_scan.set_defaults(func=cmd_scan)

    # train
    ap_train = sp.add_parser(
        "train",
        help="ì½”í¼ìŠ¤ ìƒì„± + ëª¨ë¸ í•™ìŠµ (ê¸°ë³¸: ë²ˆì—­ ë¹„í™œì„±, ë‹¤êµ­ì–´ Sentence-BERT)",
    )
    ap_train.add_argument("--scan_csv", default="./data/found_files.csv")
    ap_train.add_argument("--corpus", default="./data/corpus.parquet")
    ap_train.add_argument("--model", default="./data/topic_model.joblib")
    ap_train.add_argument("--max_features", type=int, default=50000)
    ap_train.add_argument("--n_components", type=int, default=DEFAULT_N_COMPONENTS)
    ap_train.add_argument("--n_clusters", type=int, default=25)
    ap_train.add_argument("--min_df", type=int, default=2)
    ap_train.add_argument("--max_df", type=float, default=0.85)
    ap_train.add_argument("--embedding-model", default=DEFAULT_EMBED_MODEL, help="Sentence-BERT ì„ë² ë”© ëª¨ë¸ ì´ë¦„")
    ap_train.add_argument("--embedding-batch-size", type=int, default=32, help="Sentence-BERT ë°°ì¹˜ í¬ê¸°")
    ap_train.add_argument(
        "--limit",
        "--limit-files",
        dest="limit_files",
        type=int,
        default=0,
        help="í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ìƒìœ„ Nê°œ íŒŒì¼ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤ (0=ì „ì²´).",
    )
    translate_group = ap_train.add_mutually_exclusive_group()
    translate_group.add_argument(
        "--translate",
        dest="translate",
        action="store_true",
        help="deep-translatorë¡œ ì˜ì–´ ë²ˆì—­ì„ ê°•ì œ í™œì„±í™”í•©ë‹ˆë‹¤.",
    )
    translate_group.add_argument(
        "--no-translate",
        dest="translate",
        action="store_false",
        help="ë²ˆì—­ ê¸°ëŠ¥ì„ ë¹„í™œì„±í™”í•˜ê³  ì›ë¬¸ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.",
    )
    ap_train.add_argument("--no-embedding", dest="use_embedding", action="store_false", help="Sentence-BERT ëŒ€ì‹  TF-IDF ë°±ì—… ê²½ë¡œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    ap_train.set_defaults(translate=False)
    ap_train.set_defaults(use_embedding=True)
    ap_train.set_defaults(func=cmd_train)

    # pipeline
    ap_pipe = sp.add_parser(
        "pipeline",
        help="ìŠ¤ìº” í›„ ë°”ë¡œ í•™ìŠµê¹Œì§€ ì§„í–‰ (ê¸°ë³¸: ë²ˆì—­ ë¹„í™œì„±)",
    )
    ap_pipe.add_argument("--out", default="./data/found_files.csv")
    ap_pipe.add_argument(
        "--root",
        "--roots",
        dest="roots",
        action="append",
        help="ìŠ¤ìº”í•  ë£¨íŠ¸ ë””ë ‰í„°ë¦¬. ì—¬ëŸ¬ ë²ˆ ì§€ì • ê°€ëŠ¥. ë¯¸ì§€ì • ì‹œ ì „ì²´ ìŠ¤ìº”.",
    )
    ap_pipe.add_argument("--corpus", default="./data/corpus.parquet")
    ap_pipe.add_argument("--model", default="./data/topic_model.joblib")
    ap_pipe.add_argument("--max_features", type=int, default=50000)
    ap_pipe.add_argument("--n_components", type=int, default=DEFAULT_N_COMPONENTS)
    ap_pipe.add_argument("--n_clusters", type=int, default=25)
    ap_pipe.add_argument("--min_df", type=int, default=2)
    ap_pipe.add_argument("--max_df", type=float, default=0.85)
    ap_pipe.add_argument("--embedding-model", default=DEFAULT_EMBED_MODEL, help="Sentence-BERT ì„ë² ë”© ëª¨ë¸ ì´ë¦„")
    ap_pipe.add_argument("--embedding-batch-size", type=int, default=32, help="Sentence-BERT ë°°ì¹˜ í¬ê¸°")
    ap_pipe.add_argument(
        "--limit",
        "--limit-files",
        dest="limit_files",
        type=int,
        default=0,
        help="í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ìƒìœ„ Nê°œ íŒŒì¼ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤ (0=ì „ì²´).",
    )
    translate_group_pipe = ap_pipe.add_mutually_exclusive_group()
    translate_group_pipe.add_argument(
        "--translate",
        dest="translate",
        action="store_true",
        help="deep-translatorë¡œ ì˜ì–´ ë²ˆì—­ì„ ê°•ì œ í™œì„±í™”í•©ë‹ˆë‹¤.",
    )
    translate_group_pipe.add_argument(
        "--no-translate",
        dest="translate",
        action="store_false",
        help="ë²ˆì—­ ê¸°ëŠ¥ì„ ë¹„í™œì„±í™”í•˜ê³  ì›ë¬¸ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.",
    )
    ap_pipe.add_argument("--no-embedding", dest="use_embedding", action="store_false", help="Sentence-BERT ëŒ€ì‹  TF-IDF ë°±ì—… ê²½ë¡œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    ap_pipe.set_defaults(translate=False)
    ap_pipe.set_defaults(use_embedding=True)
    ap_pipe.set_defaults(func=cmd_pipeline)

    # chat
    ap_chat = sp.add_parser(
        "chat",
        help="ëŒ€í™”í˜• ì§ˆì˜ ëª¨ë“œ (ê¸°ë³¸: ë²ˆì—­ ë¹„í™œì„±, ë‹¤êµ­ì–´ Sentence-BERT)",
    )
    ap_chat.add_argument("--model", default="./data/topic_model.joblib")
    ap_chat.add_argument("--corpus", default="./data/corpus.parquet")
    ap_chat.add_argument("--cache", default="./index_cache")
    ap_chat.add_argument("--scan_csv", default="./data/found_files.csv")
    ap_chat.add_argument("--topk", type=int, default=5)
    translate_group_chat = ap_chat.add_mutually_exclusive_group()
    translate_group_chat.add_argument(
        "--translate",
        dest="translate",
        action="store_true",
        help="deep-translatorë¡œ ì§ˆì˜ë¥¼ ì˜ì–´ ë²ˆì—­í•œ ë’¤ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
    )
    translate_group_chat.add_argument(
        "--no-translate",
        dest="translate",
        action="store_false",
        help="ì§ˆë¬¸ ë²ˆì—­ ê¸°ëŠ¥ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.",
    )
    ap_chat.add_argument("--no-auto-train", dest="auto_train", action="store_false", help="ìë™ í•™ìŠµ ê°±ì‹ ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
    rerank_group = ap_chat.add_mutually_exclusive_group()
    rerank_group.add_argument("--rerank", dest="rerank", action="store_true", help="Cross-Encoder ì¬ë­í‚¹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤ (ê¸°ë³¸ê°’).")
    rerank_group.add_argument("--no-rerank", dest="rerank", action="store_false", help="Cross-Encoder ì¬ë­í‚¹ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
    ap_chat.add_argument("--rerank-model", default="cross-encoder/ms-marco-MiniLM-L-6-v2", help="ì¬ë­í‚¹ì— ì‚¬ìš©í•  Cross-Encoder ëª¨ë¸ ì´ë¦„")
    ap_chat.add_argument("--rerank-depth", type=int, default=80, help="Cross-Encoder ì¬ë­í‚¹ì— í¬í•¨í•  í›„ë³´ ë¬¸ì„œ ìˆ˜ (50~100 ê¶Œì¥)")
    ap_chat.add_argument("--rerank-batch-size", type=int, default=16, help="Cross-Encoder ì¶”ë¡  ë°°ì¹˜ í¬ê¸° (CPU í™˜ê²½ì€ 8~16 ê¶Œì¥)")
    ap_chat.add_argument("--rerank-device", default=None, help="ì¬ë­í‚¹ ëª¨ë¸ì„ ë¡œë“œí•  ë””ë°”ì´ìŠ¤(e.g. 'cuda', 'cuda:0', 'cpu')")
    ap_chat.add_argument(
        "--rerank-min-score",
        type=float,
        default=0.35,
        help="Cross-Encoder ì ìˆ˜ê°€ ì´ ê°’ë³´ë‹¤ ë‚®ì€ ë¬¸ì„œëŠ” ì œì™¸í•©ë‹ˆë‹¤.",
    )
    ap_chat.add_argument(
        "--lexical-weight",
        type=float,
        default=0.0,
        help="BM25 ê°€ì¤‘ì¹˜ (0=ì˜ë¯¸ ê²€ìƒ‰ ì „ìš©). í•„ìš” ì‹œ ìˆ˜ë™ ì¡°ì •",
    )
    ap_chat.add_argument(
        "--min-similarity",
        type=float,
        default=0.35,
        help="ì´ ê°’ë³´ë‹¤ ë‚®ì€ ìœ ì‚¬ë„ ë¬¸ì„œëŠ” ì œì™¸í•©ë‹ˆë‹¤ (0.0~1.0).",
    )
    ap_chat.add_argument("--show-translation", action="store_true", help="ê²€ìƒ‰ ê²°ê³¼ì— ë²ˆì—­ë³¸ì„ í•¨ê»˜ í‘œì‹œí•©ë‹ˆë‹¤.")
    ap_chat.add_argument("--translation-lang", default="en", help="ë²ˆì—­ ëŒ€ìƒ ì–¸ì–´ ì½”ë“œ (ê¸°ë³¸: en)")
    ap_chat.set_defaults(translate=False)
    ap_chat.set_defaults(auto_train=True)
    ap_chat.set_defaults(rerank=True)
    ap_chat.set_defaults(func=cmd_chat)

    # watch
    ap_watch = sp.add_parser("watch", help="íŒŒì¼ ë³€ê²½ì„ ê°ì§€í•´ ì½”í¼ìŠ¤/ì¸ë±ìŠ¤ë¥¼ ì¦ë¶„ ê°±ì‹ ")
    ap_watch.add_argument("--root", "--roots", dest="roots", action="append", help="ê°ì‹œí•  ë£¨íŠ¸ ë””ë ‰í„°ë¦¬ (ì—¬ëŸ¬ ë²ˆ ì§€ì • ê°€ëŠ¥)")
    ap_watch.add_argument("--scan_csv", default="./data/found_files.csv")
    ap_watch.add_argument("--corpus", default="./data/corpus.parquet")
    ap_watch.add_argument("--model", default="./data/topic_model.joblib")
    ap_watch.add_argument("--cache", default="./index_cache")
    ap_watch.add_argument("--debounce-ms", type=int, default=2000, help="íŒŒì¼ ì´ë²¤íŠ¸ ë””ë°”ìš´ìŠ¤ ì‹œê°„(ms)")
    ap_watch.add_argument("--translate", action="store_true", help="ì¦ë¶„ ì¶”ì¶œ ì‹œ ë²ˆì—­ì„ í¬í•¨í•©ë‹ˆë‹¤.")
    ap_watch.set_defaults(func=cmd_watch)

    args = ap.parse_args()
    args.func(args)


if __name__ == "__main__":
    main()
