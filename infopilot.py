# infopilot.py
from __future__ import annotations

import argparse
import math
from pathlib import Path
from typing import Any, Dict, List, Optional

import csv


# ëª¨ë“ˆ ì„í¬íŠ¸
from filefinder import FileFinder
from pipeline import run_step2, TrainConfig, DEFAULT_N_COMPONENTS
from lnp_chat import LNPChat # ìƒˆë¡œìš´ LNP Chat í´ë˜ìŠ¤ë¥¼ ì„í¬íŠ¸


NORMALIZED_ALIASES = {
    "path": ("path", "filepath", "file_path", "fullpath", "full_path", "absolute_path"),
    "size": ("size", "filesize", "file_size", "bytes"),
    "mtime": ("mtime", "modified", "modified_time", "lastmodified", "timestamp"),
    "ext": ("ext", "extension", "suffix"),
    "drive": ("drive", "volume", "root"),
}


def _normalize_key(name: str) -> str:
    """Normalize header names by stripping non-alphanumerics and lowering case."""
    return "".join(ch for ch in (name or "").lower() if ch.isalnum())


def _pick_value(row: Dict[str, str], aliases) -> str:
    normalized = {_normalize_key(k): (k, v) for k, v in row.items() if k}
    for alias in aliases:
        alias_norm = _normalize_key(alias)
        data = normalized.get(alias_norm)
        if data:
            value = (data[1] or "").strip()
            if value:
                return value
    return ""


def _normalize_scan_row(raw: Dict[str, str], *, context: str = "") -> Dict[str, Any] | None:
    path = _pick_value(raw, NORMALIZED_ALIASES["path"])
    if not path:
        columns = ", ".join(k for k in raw.keys() if k)
        location = f" ({context})" if context else ""
        print(f"âš ï¸ ê²½ê³ : 'path' ê°’ì„ ì°¾ì§€ ëª»í•´ í–‰ì„ ê±´ë„ˆëœë‹ˆë‹¤{location}. (ê°ì§€í•œ ì—´: {columns or 'ì—†ìŒ'})")
        return None

    size_raw = _pick_value(raw, NORMALIZED_ALIASES["size"])
    mtime_raw = _pick_value(raw, NORMALIZED_ALIASES["mtime"])
    ext = _pick_value(raw, NORMALIZED_ALIASES["ext"])
    drive = _pick_value(raw, NORMALIZED_ALIASES["drive"])

    def to_int(value: str) -> int:
        try:
            return int(float(value))
        except (TypeError, ValueError):
            return 0

    def to_float(value: str) -> float:
        try:
            out = float(value)
            if math.isnan(out) or math.isinf(out):
                return 0.0
            return out
        except (TypeError, ValueError):
            return 0.0

    normalized = dict(raw)
    normalized["path"] = path
    normalized["size"] = to_int(size_raw)
    normalized["mtime"] = to_float(mtime_raw)
    if ext:
        normalized["ext"] = ext
    if drive:
        normalized["drive"] = drive
    return normalized


def _parse_roots(raw_roots: List[str] | None) -> List[Path] | None:
    if not raw_roots:
        return None
    roots: List[Path] = []
    for raw in raw_roots:
        p = Path(raw).expanduser().resolve()
        if not p.exists():
            print(f"âš ï¸ ê²½ê³ : ì§€ì •í•œ ë£¨íŠ¸ '{p}'ì´(ê°€) ì¡´ì¬í•˜ì§€ ì•Šì•„ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
        roots.append(p)
    if not roots:
        print("âš ï¸ ê²½ê³ : ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë£¨íŠ¸ê°€ ì—†ì–´ ê¸°ë³¸ ì „ì²´ ìŠ¤ìº”ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
        return None
    return roots


def _run_scan(out: Path, roots: List[Path] | None = None) -> List[Dict[str, Any]]:
    finder = FileFinder(
        exts=FileFinder.DEFAULT_EXTS,
        scan_all_drives=True,
        start_from_current_drive_only=False,
        follow_symlinks=False,
        max_depth=None,
        show_progress=True,
        progress_update_secs=0.5,
        estimate_total_dirs=False,
        startup_banner=True,
    )
    files = finder.find(roots=roots, run_async=False)
    FileFinder.to_csv(files, out)
    print(f"ğŸ“¦ ìŠ¤ìº” ê²°ê³¼ ì €ì¥: {out}")
    return files


def cmd_scan(args):
    roots = _parse_roots(args.roots)
    _run_scan(Path(args.out), roots)


def _resolve_scan_csv(path: Path) -> Path:
    if path.exists():
        return path

    search_root = path.parent if path.parent else Path(".")
    candidates = []
    for candidate in sorted(search_root.glob("*.csv"), key=lambda p: p.stat().st_mtime, reverse=True):
        try:
            with candidate.open("r", encoding="utf-8", newline="") as f:
                reader = csv.DictReader(f)
                headers = reader.fieldnames or []
        except OSError:
            continue
        header_norm = {_normalize_key(h) for h in headers}
        if any(_normalize_key(alias) in header_norm for alias in NORMALIZED_ALIASES["path"]):
            candidates.append(candidate)

    if candidates:
        picked = candidates[0]
        print(f"âš ï¸ '{path}' íŒŒì¼ì´ ì—†ì–´ '{picked}'ì„(ë¥¼) ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return picked

    raise FileNotFoundError(f"ìŠ¤ìº” CSVë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")


def _load_scan_rows(scan_csv: Path) -> List[Dict[str, Any]]:
    rows: List[Dict[str, Any]] = []
    with scan_csv.open("r", encoding="utf-8", newline="") as f:
        reader = csv.DictReader(f)
        for idx, raw in enumerate(reader, start=2):
            normalized = _normalize_scan_row(raw, context=f"{scan_csv}:{idx}")
            if normalized:
                rows.append(normalized)
    return rows


def _build_train_config(args) -> TrainConfig:
    return TrainConfig(
        max_features=args.max_features,
        n_components=args.n_components,
        n_clusters=args.n_clusters,
        ngram_range=(1, 2),
        min_df=args.min_df,
        max_df=args.max_df,
    )


def _maybe_limit_rows(rows: List[Dict[str, Any]], limit: int) -> List[Dict[str, Any]]:
    if limit and len(rows) > limit:
        print(f"âš¡ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ìƒìœ„ {limit}ê°œ íŒŒì¼ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return rows[:limit]
    return rows


def _default_train_config() -> TrainConfig:
    return TrainConfig(
        max_features=50000,
        n_components=DEFAULT_N_COMPONENTS,
        n_clusters=25,
        ngram_range=(1, 2),
        min_df=2,
        max_df=0.85,
    )


def _ensure_chat_artifacts(
    scan_csv: Path,
    corpus: Path,
    model: Path,
    *,
    translate: bool,
    auto_train: bool,
) -> bool:
    """Ensure chat artifacts exist and are up to date. Returns True if training ran."""

    def mtime(path: Path) -> float:
        try:
            return path.stat().st_mtime
        except OSError:
            return 0.0

    resolved_scan: Optional[Path] = None
    if scan_csv:
        try:
            resolved_scan = _resolve_scan_csv(scan_csv)
        except FileNotFoundError:
            resolved_scan = None

    artifacts_exist = corpus.exists() and model.exists()
    needs_train = not artifacts_exist

    if not needs_train and resolved_scan:
        scan_mtime = mtime(resolved_scan)
        artifacts_mtime = min(mtime(corpus), mtime(model))
        if scan_mtime > artifacts_mtime:
            needs_train = True

    if not needs_train:
        print("ğŸ”„ ì¸ë±ìŠ¤ ìµœì‹ ì„± í™•ì¸ ì™„ë£Œ.")
        return False

    if resolved_scan is None:
        msg = (
            "âš ï¸ í•™ìŠµ ì‚°ì¶œë¬¼ì´ ì—†ê±°ë‚˜ ì˜¤ë˜ë˜ì—ˆì§€ë§Œ ì‚¬ìš©í•  ìŠ¤ìº” CSVë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            " `--scan_csv` ê²½ë¡œë¥¼ í™•ì¸í•˜ê±°ë‚˜ 'scan' ëª…ë ¹ì„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
        )
        raise FileNotFoundError(msg)

    if not auto_train:
        raise RuntimeError(
            "í•™ìŠµ ì‚°ì¶œë¬¼ì´ ìµœì‹ ì´ ì•„ë‹™ë‹ˆë‹¤. 'python infopilot.py train --scan_csv "
            f"{resolved_scan}'ë¥¼ ì‹¤í–‰í•œ ë’¤ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        )

    print("âš ï¸ ìŠ¤ìº” ê²°ê³¼ê°€ ëª¨ë¸ë³´ë‹¤ ìµœì‹ ì…ë‹ˆë‹¤. ìë™ìœ¼ë¡œ train ë‹¨ê³„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    rows = _load_scan_rows(resolved_scan)
    if not rows:
        raise ValueError("ìë™ í•™ìŠµì„ ìœ„í•œ ìœ íš¨í•œ í–‰ì´ ì—†ìŠµë‹ˆë‹¤. ìŠ¤ìº” ê²°ê³¼ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    cfg = _default_train_config()
    run_step2(
        rows,
        out_corpus=corpus,
        out_model=model,
        cfg=cfg,
        use_tqdm=True,
        translate=translate,
    )
    print("âœ… ìë™ í•™ìŠµ ì™„ë£Œ")
    return True


def cmd_train(args):
    scan_csv = _resolve_scan_csv(Path(args.scan_csv))
    rows = _load_scan_rows(scan_csv)
    rows = _maybe_limit_rows(rows, args.limit_files)

    if not rows:
        raise ValueError("ìœ íš¨í•œ í•™ìŠµ ëŒ€ìƒ í–‰ì´ ì—†ìŠµë‹ˆë‹¤. ìŠ¤ìº” CSVë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    cfg = _build_train_config(args)
    out_corpus = Path(args.corpus)
    out_model = Path(args.model)
    df, tm = run_step2(rows, out_corpus=out_corpus, out_model=out_model, cfg=cfg, use_tqdm=True, translate=args.translate)
    print("âœ… í•™ìŠµ ì™„ë£Œ")


def cmd_pipeline(args):
    out = Path(args.out)
    roots = _parse_roots(args.roots)
    _run_scan(out, roots)
    rows = _load_scan_rows(out)
    rows = _maybe_limit_rows(rows, args.limit_files)

    if not rows:
        raise ValueError("ìœ íš¨í•œ í•™ìŠµ ëŒ€ìƒ í–‰ì´ ì—†ìŠµë‹ˆë‹¤. ìŠ¤ìº” ê²°ê³¼ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    cfg = _build_train_config(args)
    out_corpus = Path(args.corpus)
    out_model = Path(args.model)
    df, tm = run_step2(rows, out_corpus=out_corpus, out_model=out_model, cfg=cfg, use_tqdm=True, translate=args.translate)
    print("âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")


def cmd_chat(args):
    """ëŒ€í™”í˜• ê²€ìƒ‰ ëª¨ë“œ (LNPChat ì‚¬ìš©)"""
    auto_trained = _ensure_chat_artifacts(
        scan_csv=Path(args.scan_csv),
        corpus=Path(args.corpus),
        model=Path(args.model),
        translate=args.translate,
        auto_train=args.auto_train,
    )

    # LNPChat í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ì¤€ë¹„
    chat_session = LNPChat(
        model_path=Path(args.model),
        corpus_path=Path(args.corpus),
        cache_dir=Path(args.cache),
        topk=args.topk,
        translate=args.translate # ë²ˆì—­ ì˜µì…˜ ì „ë‹¬
    )
    chat_session.ready(rebuild=auto_trained)

    print("\nğŸ’¬ InfoPilot Chat ëª¨ë“œì…ë‹ˆë‹¤. (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'ì¢…ë£Œ' ì…ë ¥)")
    while True:
        try:
            query = input("ì§ˆë¬¸> ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        if not query:
            continue
        if query.lower() in {"exit", "quit", "ì¢…ë£Œ"}:
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        # LNPChatì˜ ask ë©”ì†Œë“œ í˜¸ì¶œ
        result = chat_session.ask(query)
        print(result["answer"])
        if result.get("suggestions"):
            print("\nğŸ’¡ ì´ëŸ° ì§ˆë¬¸ì€ ì–´ë– ì„¸ìš”?")
            for s in result["suggestions"]:
                print(f"   - {s}")
        print("-" * 80)


def main():
    ap = argparse.ArgumentParser(prog="infopilot", description="InfoPilot CLI - ë‹¤êµ­ì–´ ë¬¸ì„œ ê²€ìƒ‰ê¸°")
    sp = ap.add_subparsers(dest="cmd", required=True)

    # scan
    ap_scan = sp.add_parser("scan", help="ë“œë¼ì´ë¸Œ ìŠ¤ìº”í•˜ì—¬ íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘")
    ap_scan.add_argument("--out", default="./data/found_files.csv")
    ap_scan.add_argument(
        "--root",
        "--roots",
        dest="roots",
        action="append",
        help="ìŠ¤ìº”í•  ë£¨íŠ¸ ë””ë ‰í„°ë¦¬. ì—¬ëŸ¬ ë²ˆ ì§€ì • ê°€ëŠ¥. ë¯¸ì§€ì • ì‹œ ì „ì²´ ìŠ¤ìº”.",
    )
    ap_scan.set_defaults(func=cmd_scan)

    # train
    ap_train = sp.add_parser("train", help="ì½”í¼ìŠ¤ ìƒì„± + ëª¨ë¸ í•™ìŠµ (ê¸°ë³¸: ë²ˆì—­ í™œì„±)")
    ap_train.add_argument("--scan_csv", default="./data/found_files.csv")
    ap_train.add_argument("--corpus", default="./data/corpus.parquet")
    ap_train.add_argument("--model", default="./data/topic_model.joblib")
    ap_train.add_argument("--max_features", type=int, default=50000)
    ap_train.add_argument("--n_components", type=int, default=DEFAULT_N_COMPONENTS)
    ap_train.add_argument("--n_clusters", type=int, default=25)
    ap_train.add_argument("--min_df", type=int, default=2)
    ap_train.add_argument("--max_df", type=float, default=0.85)
    ap_train.add_argument(
        "--limit",
        "--limit-files",
        dest="limit_files",
        type=int,
        default=0,
        help="í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ìƒìœ„ Nê°œ íŒŒì¼ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤ (0=ì „ì²´).",
    )
    ap_train.add_argument("--no-translate", dest="translate", action="store_false", help="ë²ˆì—­ ê¸°ëŠ¥ì„ ë¹„í™œì„±í™”í•˜ê³  ì›ë¬¸ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.")
    ap_train.set_defaults(translate=True)
    ap_train.set_defaults(func=cmd_train)

    # pipeline
    ap_pipe = sp.add_parser("pipeline", help="ìŠ¤ìº” í›„ ë°”ë¡œ í•™ìŠµê¹Œì§€ ì§„í–‰")
    ap_pipe.add_argument("--out", default="./data/found_files.csv")
    ap_pipe.add_argument(
        "--root",
        "--roots",
        dest="roots",
        action="append",
        help="ìŠ¤ìº”í•  ë£¨íŠ¸ ë””ë ‰í„°ë¦¬. ì—¬ëŸ¬ ë²ˆ ì§€ì • ê°€ëŠ¥. ë¯¸ì§€ì • ì‹œ ì „ì²´ ìŠ¤ìº”.",
    )
    ap_pipe.add_argument("--corpus", default="./data/corpus.parquet")
    ap_pipe.add_argument("--model", default="./data/topic_model.joblib")
    ap_pipe.add_argument("--max_features", type=int, default=50000)
    ap_pipe.add_argument("--n_components", type=int, default=DEFAULT_N_COMPONENTS)
    ap_pipe.add_argument("--n_clusters", type=int, default=25)
    ap_pipe.add_argument("--min_df", type=int, default=2)
    ap_pipe.add_argument("--max_df", type=float, default=0.85)
    ap_pipe.add_argument(
        "--limit",
        "--limit-files",
        dest="limit_files",
        type=int,
        default=0,
        help="í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ìƒìœ„ Nê°œ íŒŒì¼ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤ (0=ì „ì²´).",
    )
    ap_pipe.add_argument("--no-translate", dest="translate", action="store_false", help="ë²ˆì—­ ê¸°ëŠ¥ì„ ë¹„í™œì„±í™”í•˜ê³  ì›ë¬¸ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.")
    ap_pipe.set_defaults(translate=True)
    ap_pipe.set_defaults(func=cmd_pipeline)

    # chat
    ap_chat = sp.add_parser("chat", help="ëŒ€í™”í˜• ì§ˆì˜ ëª¨ë“œ (ê¸°ë³¸: ë²ˆì—­ í™œì„±)")
    ap_chat.add_argument("--model", default="./data/topic_model.joblib")
    ap_chat.add_argument("--corpus", default="./data/corpus.parquet")
    ap_chat.add_argument("--cache", default="./index_cache")
    ap_chat.add_argument("--scan_csv", default="./data/found_files.csv")
    ap_chat.add_argument("--topk", type=int, default=5)
    ap_chat.add_argument("--no-translate", dest="translate", action="store_false", help="ì§ˆë¬¸ ë²ˆì—­ ê¸°ëŠ¥ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
    ap_chat.add_argument("--no-auto-train", dest="auto_train", action="store_false", help="ìë™ í•™ìŠµ ê°±ì‹ ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
    ap_chat.set_defaults(translate=True)
    ap_chat.set_defaults(auto_train=True)
    ap_chat.set_defaults(func=cmd_chat)

    args = ap.parse_args()
    args.func(args)


if __name__ == "__main__":
    main()
