# infopilot.py
from __future__ import annotations

import argparse
import json
import hashlib
import itertools
import math
import queue
import threading
import time
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Callable, Dict, Iterable, Iterator, List, Optional, Set, Tuple

ROOT_DIR = Path(__file__).resolve().parents[2]
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))

import csv

csv.field_size_limit(10**7)  # 10MBê¹Œì§€ í—ˆìš©

import numpy as np

try:
    import pandas as pd
except Exception:
    pd = None

try:
    from sentence_transformers import SentenceTransformer
except Exception:
    SentenceTransformer = None

try:
    import joblib
except Exception:
    joblib = None

try:
    from watchdog.events import FileSystemEventHandler
    from watchdog.observers import Observer
except Exception:
    FileSystemEventHandler = object  # type: ignore
    Observer = None


# ëª¨ë“ˆ ì„í¬íŠ¸
from core.config.paths import (
    CACHE_DIR,
    CORPUS_PATH,
    DATA_DIR,
    TOPIC_MODEL_PATH,
)
from core.data_pipeline.filefinder import FileFinder
from core.data_pipeline.policies.engine import PolicyEngine, SmartFolderPolicy
from core.data_pipeline.pipeline import (
    run_step2,
    TrainConfig,
    DEFAULT_N_COMPONENTS,
    DEFAULT_EMBED_MODEL,
    update_corpus_file,
    remove_from_corpus,
    CorpusBuilder,
)
from core.infra.scheduler import JobScheduler, ScheduleSpec, ScheduledJob
from core.infra.models import ModelManager
from core.conversation.lnp_chat import LNPChat
from core.search.retriever import (
    VectorIndex,
    MODEL_TEXT_COLUMN,
    _split_tokens,
)


KNOWLEDGE_AGENT = "knowledge_search"
DEFAULT_POLICY_PATH = Path("./config/smart_folders.json")
DEFAULT_FOUND_FILES = DATA_DIR / "found_files.csv"
DEFAULT_SCHEDULED_ROOT = DATA_DIR / "scheduled"

_POLICY_CACHE: Dict[Path, PolicyEngine] = {}
_SENTENCE_ENCODER_MANAGER: Optional[ModelManager] = None


NORMALIZED_ALIASES = {
    "path": ("path", "filepath", "file_path", "fullpath", "full_path", "absolute_path"),
    "size": ("size", "filesize", "file_size", "bytes"),
    "mtime": ("mtime", "modified", "modified_time", "lastmodified", "timestamp"),
    "ctime": ("ctime", "created", "created_time", "creation", "creation_time"),
    "ext": ("ext", "extension", "suffix"),
    "drive": ("drive", "volume", "root"),
    "owner": ("owner", "user", "username", "author", "created_by"),
}


def _get_sentence_encoder_manager() -> ModelManager:
    global _SENTENCE_ENCODER_MANAGER
    if _SENTENCE_ENCODER_MANAGER is None:
        def _load(model_name: str):
            if SentenceTransformer is None:
                raise RuntimeError("sentence-transformers íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install sentence-transformers")
            return SentenceTransformer(model_name)

        _SENTENCE_ENCODER_MANAGER = ModelManager(loader=_load)
    return _SENTENCE_ENCODER_MANAGER


def _normalize_key(name: str) -> str:
    """Normalize header names by stripping non-alphanumerics and lowering case."""
    return "".join(ch for ch in (name or "").lower() if ch.isalnum())


def _pick_value(row: Dict[str, str], aliases) -> str:
    normalized = {_normalize_key(k): (k, v) for k, v in row.items() if k}
    for alias in aliases:
        alias_norm = _normalize_key(alias)
        data = normalized.get(alias_norm)
        if data:
            value = (data[1] or "").strip()
            if value:
                return value
    return ""


def _normalize_scan_row(raw: Dict[str, str], *, context: str = "") -> Dict[str, Any] | None:
    path = _pick_value(raw, NORMALIZED_ALIASES["path"])
    if not path:
        columns = ", ".join(k for k in raw.keys() if k)
        location = f" ({context})" if context else ""
        print(f"âš ï¸ ê²½ê³ : 'path' ê°’ì„ ì°¾ì§€ ëª»í•´ í–‰ì„ ê±´ë„ˆëœë‹ˆë‹¤{location}. (ê°ì§€í•œ ì—´: {columns or 'ì—†ìŒ'})")
        return None

    size_raw = _pick_value(raw, NORMALIZED_ALIASES["size"])
    mtime_raw = _pick_value(raw, NORMALIZED_ALIASES["mtime"])
    ext = _pick_value(raw, NORMALIZED_ALIASES["ext"])
    drive = _pick_value(raw, NORMALIZED_ALIASES["drive"])
    ctime_raw = _pick_value(raw, NORMALIZED_ALIASES["ctime"])
    owner = _pick_value(raw, NORMALIZED_ALIASES["owner"])

    def to_int(value: str) -> int:
        try:
            return int(float(value))
        except (TypeError, ValueError):
            return 0

    def to_float(value: str) -> float:
        try:
            out = float(value)
            if math.isnan(out) or math.isinf(out):
                return 0.0
            return out
        except (TypeError, ValueError):
            return 0.0

    normalized = dict(raw)
    normalized["path"] = path
    normalized["size"] = to_int(size_raw)
    normalized["mtime"] = to_float(mtime_raw)
    normalized["ctime"] = to_float(ctime_raw)
    if ext:
        normalized["ext"] = ext
    if drive:
        normalized["drive"] = drive
    if owner:
        normalized["owner"] = owner
    return normalized


def _parse_roots(raw_roots: List[str] | None) -> List[Path] | None:
    if not raw_roots:
        return None
    roots: List[Path] = []
    for raw in raw_roots:
        p = Path(raw).expanduser().resolve()
        if not p.exists():
            print(f"âš ï¸ ê²½ê³ : ì§€ì •í•œ ë£¨íŠ¸ '{p}'ì´(ê°€) ì¡´ì¬í•˜ì§€ ì•Šì•„ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
        roots.append(p)
    if not roots:
        print("âš ï¸ ê²½ê³ : ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë£¨íŠ¸ê°€ ì—†ì–´ ê¸°ë³¸ ì „ì²´ ìŠ¤ìº”ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
        return None
    return roots


def _load_policy_engine(policy_arg: Optional[str]) -> PolicyEngine:
    raw = (policy_arg or str(DEFAULT_POLICY_PATH)).strip()
    if raw.lower() == "none" or raw == "":
        return PolicyEngine.empty()
    path = Path(raw).expanduser()
    try:
        cache_key = path.resolve()
    except OSError:
        cache_key = path
    engine = _POLICY_CACHE.get(cache_key)
    if engine is None:
        try:
            engine = PolicyEngine.from_file(path)
        except Exception as exc:
            print(f"âš ï¸ ì •ì±… íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {exc}")
            engine = PolicyEngine.empty()
        _POLICY_CACHE[cache_key] = engine
    return engine


def _run_scan(
    out: Path,
    roots: List[Path] | None = None,
    *,
    policy_engine: Optional[PolicyEngine] = None,
) -> List[Dict[str, Any]]:
    scan_roots = roots
    if policy_engine and policy_engine.has_policies and not roots:
        candidate_roots = policy_engine.roots_for_agent(KNOWLEDGE_AGENT, include_manual=True)
        if candidate_roots:
            scan_roots = candidate_roots
            print("ğŸ“ ì •ì±… ê¸°ë°˜ ìŠ¤ìº” ë£¨íŠ¸:")
            for root in candidate_roots:
                print(f"   - {root}")

    finder = FileFinder(
        exts=FileFinder.DEFAULT_EXTS,
        scan_all_drives=True,
        start_from_current_drive_only=False,
        follow_symlinks=False,
        max_depth=None,
        show_progress=True,
        progress_update_secs=0.5,
        estimate_total_dirs=False,
        startup_banner=True,
    )
    files = finder.find(roots=scan_roots, run_async=False)
    if policy_engine and policy_engine.has_policies:
        files = policy_engine.filter_records(files, agent=KNOWLEDGE_AGENT, include_manual=True)
    FileFinder.to_csv(files, out)
    print(f"ğŸ“¦ ìŠ¤ìº” ê²°ê³¼ ì €ì¥: {out}")
    return files


def cmd_scan(args):
    policy_engine = _load_policy_engine(getattr(args, "policy", None))
    roots = _parse_roots(args.roots)
    _run_scan(Path(args.out), roots, policy_engine=policy_engine)


def _resolve_scan_csv(path: Path) -> Path:
    if path.exists():
        return path

    search_root = path.parent if path.parent else Path(".")
    candidates = []
    for candidate in sorted(search_root.glob("*.csv"), key=lambda p: p.stat().st_mtime, reverse=True):
        try:
            with candidate.open("r", encoding="utf-8", newline="") as f:
                reader = csv.DictReader(f)
                headers = reader.fieldnames or []
        except OSError:
            continue
        header_norm = {_normalize_key(h) for h in headers}
        if any(_normalize_key(alias) in header_norm for alias in NORMALIZED_ALIASES["path"]):
            candidates.append(candidate)

    if candidates:
        picked = candidates[0]
        print(f"âš ï¸ '{path}' íŒŒì¼ì´ ì—†ì–´ '{picked}'ì„(ë¥¼) ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return picked

    raise FileNotFoundError(f"ìŠ¤ìº” CSVë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")


def _iter_scan_rows(scan_csv: Path) -> Iterator[Dict[str, Any]]:
    with scan_csv.open("r", encoding="utf-8", newline="") as f:
        reader = csv.DictReader(f)
        for idx, raw in enumerate(reader, start=2):
            normalized = _normalize_scan_row(raw, context=f"{scan_csv}:{idx}")
            if normalized:
                yield normalized


def _load_scan_rows(
    scan_csv: Path,
    *,
    policy_engine: Optional[PolicyEngine] = None,
    include_manual: bool = True,
) -> Iterator[Dict[str, Any]]:
    for row in _iter_scan_rows(scan_csv):
        if policy_engine and policy_engine.has_policies:
            raw_path = row.get("path")
            if not raw_path:
                continue
            if not policy_engine.allows(
                Path(str(raw_path)),
                agent=KNOWLEDGE_AGENT,
                include_manual=include_manual,
            ):
                continue
        yield row


@dataclass
class _PolicyArtifacts:
    base_dir: Path
    scan_csv: Path
    corpus: Path
    model: Path
    cache_dir: Path

    def ensure_dirs(self) -> None:
        self.base_dir.mkdir(parents=True, exist_ok=True)
        self.cache_dir.mkdir(parents=True, exist_ok=True)


def _policy_slug(policy: SmartFolderPolicy) -> str:
    digest = hashlib.sha1(str(policy.path).encode("utf-8")).hexdigest()[:8]
    candidate = policy.path.name or policy.path.anchor.strip("\\/") or "policy"
    safe = "".join(ch.lower() if ch.isalnum() else "_" for ch in candidate).strip("_") or "policy"
    return f"{safe}-{digest}"


def _policy_artifacts(root: Path, policy: SmartFolderPolicy) -> _PolicyArtifacts:
    slug = _policy_slug(policy)
    base_dir = root / slug
    return _PolicyArtifacts(
        base_dir=base_dir,
        scan_csv=base_dir / "found_files.csv",
        corpus=base_dir / "corpus.parquet",
        model=base_dir / "topic_model.joblib",
        cache_dir=base_dir / "cache",
    )


def _sync_scan_csv(
    scan_csv: Path,
    rows_to_add: List[Dict[str, Any]],
    paths_to_remove: Set[str],
) -> None:
    if not rows_to_add and not paths_to_remove:
        return

    def _normalize_path(raw: Any) -> str:
        return str(raw or "").strip()

    fieldnames = ["path", "size", "mtime", "ctime", "ext", "drive", "owner"]
    additions: Dict[str, Dict[str, Any]] = {}
    for row in rows_to_add:
        path_key = _normalize_path(row.get("path"))
        if not path_key:
            continue
        additions[path_key] = {name: row.get(name) for name in fieldnames}

    removals = {_normalize_path(path) for path in paths_to_remove if _normalize_path(path)}
    removals.difference_update(additions.keys())

    scan_csv.parent.mkdir(parents=True, exist_ok=True)

    if not scan_csv.exists():
        with scan_csv.open("w", encoding="utf-8", newline="") as dst:
            writer = csv.DictWriter(dst, fieldnames=fieldnames)
            writer.writeheader()
            for record in additions.values():
                writer.writerow(record)
        return

    temp_path = scan_csv.with_suffix(scan_csv.suffix + ".tmp")
    with scan_csv.open("r", encoding="utf-8", newline="") as src, temp_path.open(
        "w", encoding="utf-8", newline=""
    ) as dst:
        reader = csv.DictReader(src)
        writer = csv.DictWriter(dst, fieldnames=fieldnames)
        writer.writeheader()

        for row in reader:
            path_key = _normalize_path(row.get("path"))
            if not path_key or path_key in removals or path_key in additions:
                continue
            writer.writerow({name: row.get(name) for name in fieldnames})

        for record in additions.values():
            writer.writerow(record)

    temp_path.replace(scan_csv)


def _load_sentence_encoder(model_path: Path) -> Tuple[Optional[SentenceTransformer], int, str]:
    model_name = DEFAULT_EMBED_MODEL
    batch_size = 32

    if joblib is not None and model_path.exists():
        try:
            payload = joblib.load(model_path)
            model_name = payload.get("model_name", model_name)
            cfg = payload.get("train_config")
            if cfg and hasattr(cfg, "embedding_batch_size"):
                batch_size = int(getattr(cfg, "embedding_batch_size", batch_size) or batch_size)
        except Exception as exc:
            print(f"âš ï¸ ì„ë² ë”© ëª¨ë¸ ë©”íƒ€ ë¡œë“œ ì‹¤íŒ¨ â†’ ê¸°ë³¸ê°’ ì‚¬ìš©({model_name}): {exc}")

    try:
        manager = _get_sentence_encoder_manager()
    except RuntimeError as exc:
        print(f"âš ï¸ SentenceTransformer ë¡œë” ì´ˆê¸°í™” ì‹¤íŒ¨: {exc}")
        return None, batch_size, model_name

    try:
        encoder = manager.get(model_name)
    except Exception as exc:
        print(f"âš ï¸ SentenceTransformer ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨({model_name}): {exc}")
        return None, batch_size, model_name
    return encoder, batch_size, model_name


def _load_vector_index(cache_dir: Path) -> VectorIndex:
    cache_dir.mkdir(parents=True, exist_ok=True)
    meta = cache_dir / "doc_meta.json"
    emb = cache_dir / "doc_embeddings.npy"
    faiss_path = cache_dir / "doc_index.faiss"

    index = VectorIndex()
    if meta.exists():
        try:
            index.load(
                emb if emb.exists() else None,
                meta,
                faiss_path=faiss_path if faiss_path.exists() else None,
                use_mmap=False,
            )
        except Exception as exc:
            print(f"âš ï¸ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨ â†’ ìƒˆ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤: {exc}")
            index = VectorIndex()
    return index


class WatchEventHandler(FileSystemEventHandler):
    def __init__(
        self,
        event_queue: "queue.Queue[Tuple[str, str]]",
        allowed_exts: Set[str],
        *,
        policy_engine: Optional[PolicyEngine] = None,
        agent: str = KNOWLEDGE_AGENT,
    ) -> None:
        super().__init__()
        self._queue = event_queue
        self._allowed_exts = {ext.lower() for ext in allowed_exts}
        self._policy_engine = policy_engine
        self._policy_agent = agent

    def _should_process(self, path: str) -> bool:
        if not path:
            return False
        ext = Path(path).suffix.lower()
        if ext not in self._allowed_exts:
            return False
        if self._policy_engine and self._policy_engine.has_policies and not self._policy_engine.allows(
            Path(path), agent=self._policy_agent, include_manual=False
        ):
            return False
        return True

    def on_created(self, event):  # type: ignore[override]
        if getattr(event, "is_directory", False):
            return
        if self._should_process(event.src_path):
            self._queue.put(("created", event.src_path))

    def on_modified(self, event):  # type: ignore[override]
        if getattr(event, "is_directory", False):
            return
        if self._should_process(event.src_path):
            self._queue.put(("modified", event.src_path))

    def on_moved(self, event):  # type: ignore[override]
        if getattr(event, "is_directory", False):
            return
        if self._should_process(event.src_path):
            self._queue.put(("deleted", event.src_path))
        if self._should_process(event.dest_path):
            self._queue.put(("created", event.dest_path))

    def on_deleted(self, event):  # type: ignore[override]
        if getattr(event, "is_directory", False):
            return
        if self._should_process(event.src_path):
            self._queue.put(("deleted", event.src_path))


class IncrementalPipeline:
    def __init__(
        self,
        *,
        encoder: SentenceTransformer,
        batch_size: int,
        scan_csv: Path,
        corpus_path: Path,
        cache_dir: Path,
        translate: bool,
        policy_engine: Optional[PolicyEngine] = None,
    ) -> None:
        self.encoder = encoder
        self.batch_size = max(1, int(batch_size))
        self.scan_csv = scan_csv
        self.corpus_path = corpus_path
        self.cache_dir = cache_dir
        self.translate = translate
        self.allowed_exts = {ext.lower() for ext in FileFinder.DEFAULT_EXTS}
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.policy_engine = policy_engine
        self.policy_agent = KNOWLEDGE_AGENT

    def process(self, add_paths: Set[str], remove_paths: Set[str]) -> None:
        if pd is None:
            raise RuntimeError("pandas í•„ìš”. pip install pandas")
        add_paths = {p for p in add_paths if Path(p).suffix.lower() in self.allowed_exts}
        remove_paths = {p for p in remove_paths if Path(p).suffix.lower() in self.allowed_exts}
        if self.policy_engine and self.policy_engine.has_policies and add_paths:
            add_paths = {
                p
                for p in add_paths
                if self.policy_engine.allows(Path(p), agent=self.policy_agent, include_manual=False)
            }

        rows_to_add: List[Dict[str, Any]] = []
        for raw_path in sorted(add_paths):
            if self.policy_engine and self.policy_engine.has_policies and not self.policy_engine.allows(
                Path(raw_path), agent=self.policy_agent, include_manual=False
            ):
                continue
            meta = FileFinder.collect_file_metadata(Path(raw_path), allowed_exts=self.allowed_exts)
            if meta:
                rows_to_add.append(meta)

        _sync_scan_csv(self.scan_csv, rows_to_add, {str(p) for p in remove_paths})

        if remove_paths:
            remove_from_corpus(list(remove_paths), self.corpus_path)

        new_records = None
        if rows_to_add:
            cb = CorpusBuilder(progress=False, translate=self.translate)
            new_records = cb.build(rows_to_add)
        else:
            new_records = None

        if new_records is not None and not new_records.empty:
            update_corpus_file(new_records, self.corpus_path)

        index = _load_vector_index(self.cache_dir)

        paths_to_remove = set(remove_paths)
        paths_to_remove.update(row["path"] for row in rows_to_add if "path" in row)
        if paths_to_remove:
            index.remove_paths(paths_to_remove)

        if new_records is None or new_records.empty:
            index.save(self.cache_dir)
            if rows_to_add or remove_paths:
                print(
                    f"âš¡ watcher: removed {len(paths_to_remove)} ë¬¸ì„œ, ìƒˆ ë¬¸ì„œ ì—†ìŒ.",
                    flush=True,
                )
            return

        valid_mask = new_records.get("ok", True)
        if pd is not None and isinstance(valid_mask, pd.Series):
            valid_df = new_records[valid_mask & (new_records[MODEL_TEXT_COLUMN].astype(str).str.len() > 0)].copy()
        else:
            valid_df = new_records.copy()

        if valid_df.empty:
            index.save(self.cache_dir)
            print(
                f"âš¡ watcher: ê°±ì‹  {len(rows_to_add)}ê±´ ì¤‘ ìœ íš¨ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.",
                flush=True,
            )
            return

        texts = valid_df[MODEL_TEXT_COLUMN].astype(str).tolist()
        embeddings = self.encoder.encode(
            texts,
            batch_size=self.batch_size,
            show_progress_bar=False,
            convert_to_numpy=True,
            normalize_embeddings=False,
        )
        embeddings = np.asarray(embeddings, dtype=np.float32)

        token_lists = [[tok for tok in _split_tokens(text.lower()) if tok] for text in texts]
        previews_series = valid_df["text_original"] if "text_original" in valid_df.columns else valid_df["text"]
        previews = previews_series.fillna("").astype(str).tolist()

        for idx, (_, row) in enumerate(valid_df.iterrows()):
            index.upsert(
                path=str(row.get("path", "")),
                ext=str(row.get("ext", "")),
                embedding=embeddings[idx],
                preview=previews[idx],
                size=int(row.get("size", 0) or 0),
                mtime=float(row.get("mtime", 0.0) or 0.0),
                ctime=float(row.get("ctime", 0.0) or 0.0),
                owner=str(row.get("owner", "") or ""),
                tokens=token_lists[idx],
            )

        index.save(self.cache_dir)
        print(
            f"âš¡ watcher: ë¬¸ì„œ {len(valid_df)}ê±´ ì—…ë°ì´íŠ¸ (ì œê±° {len(paths_to_remove)})",
            flush=True,
        )


def _watch_loop(
    event_queue: "queue.Queue[Tuple[str, str]]",
    pipeline_ctx: IncrementalPipeline,
    stop_event: threading.Event,
    debounce_sec: float,
) -> None:
    pending_add: Set[str] = set()
    pending_remove: Set[str] = set()
    last_event = 0.0

    def _log_throughput(add_count: int, remove_count: int, elapsed: float) -> None:
        total = add_count + remove_count
        if total <= 0:
            return
        rate = total / elapsed if elapsed > 0 else 0.0
        print(
            (
                "âš™ï¸ watcher: processed add={add} remove={rem} in {secs:.2f}s "
                "(~{rate:.1f}/s)"
            ).format(add=add_count, rem=remove_count, secs=elapsed, rate=rate),
            flush=True,
        )

    while not stop_event.is_set():
        try:
            event_type, path = event_queue.get(timeout=0.5)
            path = str(path)
            if event_type == "deleted":
                pending_remove.add(path)
                pending_add.discard(path)
            else:
                pending_add.add(path)
                pending_remove.discard(path)
            last_event = time.time()
        except queue.Empty:
            pass

        now = time.time()
        if (pending_add or pending_remove) and (now - last_event) >= debounce_sec:
            to_add = set(pending_add)
            to_remove = set(pending_remove)
            pending_add.clear()
            pending_remove.clear()
            try:
                t0 = time.time()
                pipeline_ctx.process(to_add, to_remove)
                _log_throughput(len(to_add), len(to_remove), time.time() - t0)
            except Exception as exc:
                print(f"âš ï¸ ì¦ë¶„ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {exc}")

    # Flush remaining events
    if pending_add or pending_remove:
        try:
            to_add = set(pending_add)
            to_remove = set(pending_remove)
            t0 = time.time()
            pipeline_ctx.process(to_add, to_remove)
            _log_throughput(len(to_add), len(to_remove), time.time() - t0)
        except Exception as exc:
            print(f"âš ï¸ ì¦ë¶„ íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {exc}")


def _register_policy_jobs(
    scheduler: JobScheduler,
    *,
    policy_engine: PolicyEngine,
    agent: str,
    output_root: Path,
    translate: bool,
) -> List[ScheduledJob]:
    if not policy_engine or not policy_engine.has_policies:
        return []

    registered: List[ScheduledJob] = []
    output_root = output_root.expanduser()

    for policy in policy_engine.iter_policies():
        if not policy.allows_agent(agent):
            continue
        schedule = ScheduleSpec.from_policy(policy)
        if schedule.mode != "scheduled":
            continue

        artifacts = _policy_artifacts(output_root, policy)

        def _job(policy=policy, artifacts=artifacts) -> None:
            artifacts.ensure_dirs()
            rows = _run_scan(artifacts.scan_csv, [policy.path], policy_engine=policy_engine)
            filtered = policy_engine.filter_records(rows, agent=agent, include_manual=True)
            if not filtered and rows:
                filtered = rows
            if not filtered:
                print(f"âš ï¸ ìŠ¤ì¼€ì¤„ëŸ¬: {policy.path}ì— ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                return
            cfg = _default_train_config()
            run_step2(
                filtered,
                out_corpus=artifacts.corpus,
                out_model=artifacts.model,
                cfg=cfg,
                use_tqdm=False,
                translate=translate,
            )
            print(f"âœ… ìŠ¤ì¼€ì¤„ëŸ¬: {policy.path} í•™ìŠµ ì™„ë£Œ â†’ {artifacts.base_dir}")

        job_name = f"{agent}:{_policy_slug(policy)}"
        metadata = {
            "path": str(policy.path),
            "artifact_dir": str(artifacts.base_dir),
            "mode": schedule.mode,
        }
        job = scheduler.register_callable(
            job_name,
            _job,
            schedule,
            metadata=metadata,
            overwrite=True,
        )
        registered.append(job)

    return registered


def _build_train_config(args) -> TrainConfig:
    return TrainConfig(
        max_features=args.max_features,
        n_components=args.n_components,
        n_clusters=args.n_clusters,
        ngram_range=(1, 2),
        min_df=args.min_df,
        max_df=args.max_df,
        use_sentence_transformer=getattr(args, "use_embedding", True),
        embedding_model=getattr(args, "embedding_model", DEFAULT_EMBED_MODEL),
        embedding_batch_size=getattr(args, "embedding_batch_size", 32),
    )


def _maybe_limit_rows(rows: Iterable[Dict[str, Any]], limit: int) -> List[Dict[str, Any]]:
    iterator = iter(rows)
    if limit and limit > 0:
        limited = list(itertools.islice(iterator, limit))
        if next(iterator, None) is not None:
            print(f"âš¡ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ìƒìœ„ {limit}ê°œ íŒŒì¼ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return limited
    return list(iterator)


def _default_train_config() -> TrainConfig:
    return TrainConfig(
        max_features=50000,
        n_components=DEFAULT_N_COMPONENTS,
        n_clusters=25,
        ngram_range=(1, 2),
        min_df=2,
        max_df=0.85,
        use_sentence_transformer=True,
        embedding_model=DEFAULT_EMBED_MODEL,
        embedding_batch_size=32,
    )


def _ensure_chat_artifacts(
    scan_csv: Path,
    corpus: Path,
    model: Path,
    *,
    translate: bool,
    auto_train: bool,
    policy_engine: Optional[PolicyEngine],
) -> bool:
    """Ensure chat artifacts exist and are up to date. Returns True if training ran."""

    def mtime(path: Path) -> float:
        try:
            return path.stat().st_mtime
        except OSError:
            return 0.0

    resolved_scan: Optional[Path] = None
    if scan_csv:
        try:
            resolved_scan = _resolve_scan_csv(scan_csv)
        except FileNotFoundError:
            resolved_scan = None

    artifacts_exist = corpus.exists() and model.exists()
    needs_train = not artifacts_exist

    if not needs_train and resolved_scan:
        scan_mtime = mtime(resolved_scan)
        artifacts_mtime = min(mtime(corpus), mtime(model))
        if scan_mtime > artifacts_mtime:
            needs_train = True

    if not needs_train:
        print("ğŸ”„ ì¸ë±ìŠ¤ ìµœì‹ ì„± í™•ì¸ ì™„ë£Œ.")
        return False

    if resolved_scan is None:
        msg = (
            "âš ï¸ í•™ìŠµ ì‚°ì¶œë¬¼ì´ ì—†ê±°ë‚˜ ì˜¤ë˜ë˜ì—ˆì§€ë§Œ ì‚¬ìš©í•  ìŠ¤ìº” CSVë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            " `--scan_csv` ê²½ë¡œë¥¼ í™•ì¸í•˜ê±°ë‚˜ 'scan' ëª…ë ¹ì„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
        )
        raise FileNotFoundError(msg)

    if not auto_train:
        raise RuntimeError(
            "í•™ìŠµ ì‚°ì¶œë¬¼ì´ ìµœì‹ ì´ ì•„ë‹™ë‹ˆë‹¤. 'python infopilot.py train --scan_csv "
            f"{resolved_scan}'ë¥¼ ì‹¤í–‰í•œ ë’¤ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        )

    print("âš ï¸ ìŠ¤ìº” ê²°ê³¼ê°€ ëª¨ë¸ë³´ë‹¤ ìµœì‹ ì…ë‹ˆë‹¤. ìë™ìœ¼ë¡œ train ë‹¨ê³„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    rows = list(
        _load_scan_rows(
            resolved_scan,
            policy_engine=policy_engine,
            include_manual=False,
        )
    )
    if not rows:
        raise ValueError("ìë™ í•™ìŠµì„ ìœ„í•œ ìœ íš¨í•œ í–‰ì´ ì—†ìŠµë‹ˆë‹¤. ìŠ¤ìº” ê²°ê³¼ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    cfg = _default_train_config()
    run_step2(
        rows,
        out_corpus=corpus,
        out_model=model,
        cfg=cfg,
        use_tqdm=True,
        translate=translate,
    )
    print("âœ… ìë™ í•™ìŠµ ì™„ë£Œ")
    return True


def cmd_train(args):
    scan_csv = _resolve_scan_csv(Path(args.scan_csv))
    policy_engine = _load_policy_engine(getattr(args, "policy", None))
    row_iter = _load_scan_rows(scan_csv, policy_engine=policy_engine, include_manual=True)
    rows = _maybe_limit_rows(row_iter, args.limit_files)

    if not rows:
        raise ValueError("ìœ íš¨í•œ í•™ìŠµ ëŒ€ìƒ í–‰ì´ ì—†ìŠµë‹ˆë‹¤. ìŠ¤ìº” CSVë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    cfg = _build_train_config(args)
    out_corpus = Path(args.corpus)
    out_model = Path(args.model)
    df, tm = run_step2(rows, out_corpus=out_corpus, out_model=out_model, cfg=cfg, use_tqdm=True, translate=args.translate)
    print("âœ… í•™ìŠµ ì™„ë£Œ")


def cmd_pipeline(args):
    out = Path(args.out)
    roots = _parse_roots(args.roots)
    policy_engine = _load_policy_engine(getattr(args, "policy", None))
    scan_rows = _run_scan(out, roots, policy_engine=policy_engine)
    filtered_rows = (
        scan_rows
        if not policy_engine or not policy_engine.has_policies
        else policy_engine.filter_records(scan_rows, agent=KNOWLEDGE_AGENT, include_manual=True)
    )
    rows = _maybe_limit_rows(filtered_rows, args.limit_files)

    if not rows:
        raise ValueError("ìœ íš¨í•œ í•™ìŠµ ëŒ€ìƒ í–‰ì´ ì—†ìŠµë‹ˆë‹¤. ìŠ¤ìº” ê²°ê³¼ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    cfg = _build_train_config(args)
    out_corpus = Path(args.corpus)
    out_model = Path(args.model)
    df, tm = run_step2(rows, out_corpus=out_corpus, out_model=out_model, cfg=cfg, use_tqdm=True, translate=args.translate)
    print("âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")

    cache_dir = Path(args.cache)
    cache_dir.mkdir(parents=True, exist_ok=True)
    print(
        "â„¹ï¸ íŒŒì´í”„ë¼ì¸ì€ scan/train ë‹¨ê³„ê¹Œì§€ë§Œ ìë™ ì‹¤í–‰ë˜ë©° chat ëª¨ë“œëŠ” ë³„ë„ ì‹¤í–‰ì´ í•„ìš”í•©ë‹ˆë‹¤.\n"
        f"   â†’ python infopilot.py chat --model {out_model} --corpus {out_corpus} --cache {cache_dir}"
    )

    if getattr(args, "launch_chat", False):
        print("\nğŸ’¬ ë°”ë¡œ chat ëª¨ë“œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤. (ì¢…ë£Œí•˜ë ¤ë©´ 'exit')")
        chat_args = argparse.Namespace(
            model=str(out_model),
            corpus=str(out_corpus),
            cache=str(cache_dir),
            scan_csv=str(out),
            topk=5,
            translate=args.translate,
            auto_train=True,
            rerank=True,
            rerank_model="cross-encoder/ms-marco-MiniLM-L-6-v2",
            rerank_depth=80,
            rerank_batch_size=16,
            rerank_device=None,
            rerank_min_score=0.35,
            lexical_weight=0.0,
            show_translation=False,
            translation_lang="en",
            min_similarity=0.35,
            policy=str(getattr(args, "policy", str(DEFAULT_POLICY_PATH))),
        )
        cmd_chat(chat_args)


def cmd_chat(args):
    """ëŒ€í™”í˜• ê²€ìƒ‰ ëª¨ë“œ (LNPChat ì‚¬ìš©)"""
    policy_engine = _load_policy_engine(getattr(args, "policy", None))
    auto_trained = _ensure_chat_artifacts(
        scan_csv=Path(args.scan_csv),
        corpus=Path(args.corpus),
        model=Path(args.model),
        translate=args.translate,
        auto_train=args.auto_train,
        policy_engine=policy_engine,
    )

    # LNPChat í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ì¤€ë¹„
    chat_session = LNPChat(
        model_path=Path(args.model),
        corpus_path=Path(args.corpus),
        cache_dir=Path(args.cache),
        topk=args.topk,
        translate=args.translate, # ë²ˆì—­ ì˜µì…˜ ì „ë‹¬
        rerank=args.rerank,
        rerank_model=args.rerank_model,
        rerank_depth=args.rerank_depth,
        rerank_batch_size=args.rerank_batch_size,
        rerank_device=args.rerank_device or None,
        rerank_min_score=args.rerank_min_score,
        lexical_weight=args.lexical_weight,
        show_translation=args.show_translation,
        translation_lang=args.translation_lang,
        min_similarity=args.min_similarity,
        policy_engine=policy_engine if policy_engine and policy_engine.has_policies else policy_engine,
        policy_scope=(getattr(args, "scope", "auto") or "auto").lower(),
        policy_agent=KNOWLEDGE_AGENT,
    )
    chat_session.ready(rebuild=auto_trained)

    single_query = getattr(args, "query", None)
    json_mode = bool(getattr(args, "json", False))
    if json_mode and not single_query:
        raise SystemExit("--json ì˜µì…˜ì€ --queryì™€ í•¨ê»˜ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.")

    if single_query:
        result = chat_session.ask(single_query)
        if json_mode:
            payload = {
                "query": single_query,
                "answer": result.get("answer"),
                "suggestions": result.get("suggestions", []),
                "results": [],
            }
            for hit in result.get("hits", [])[: args.topk]:
                payload["results"].append(
                    {
                        "title": Path(str(hit.get("path") or "")).name,
                        "path": hit.get("path"),
                        "ext": hit.get("ext"),
                        "score": hit.get("similarity", hit.get("vector_similarity")),
                        "vector_score": hit.get("vector_similarity"),
                        "lexical_score": hit.get("lexical_score"),
                        "match_reasons": hit.get("match_reasons") or [],
                        "preview": hit.get("preview"),
                        "translation": hit.get("translation"),
                    }
                )
            print(json.dumps(payload, ensure_ascii=False))
        else:
            print(result.get("answer", ""))
            suggestions = result.get("suggestions") or []
            if suggestions:
                print("\nğŸ’¡ ì´ëŸ° ì§ˆë¬¸ì€ ì–´ë– ì„¸ìš”?")
                for s in suggestions:
                    print(f"   - {s}")
        return

    print("\nğŸ’¬ InfoPilot Chat ëª¨ë“œì…ë‹ˆë‹¤. (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'ì¢…ë£Œ' ì…ë ¥)")
    while True:
        try:
            query = input("ì§ˆë¬¸> ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        if not query:
            continue
        if query.lower() in {"exit", "quit", "ì¢…ë£Œ"}:
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        # LNPChatì˜ ask ë©”ì†Œë“œ í˜¸ì¶œ
        result = chat_session.ask(query)
        print(result["answer"])
        if result.get("suggestions"):
            print("\nğŸ’¡ ì´ëŸ° ì§ˆë¬¸ì€ ì–´ë– ì„¸ìš”?")
            for s in result["suggestions"]:
                print(f"   - {s}")
        print("-" * 80)


def cmd_watch(args):
    if Observer is None:
        raise RuntimeError("watchdog ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install watchdog")

    encoder, batch_size, model_name = _load_sentence_encoder(Path(args.model))
    if encoder is None:
        raise RuntimeError("sentence-transformers ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ì–´ watcherë¥¼ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    policy_engine = _load_policy_engine(getattr(args, "policy", None))
    roots = _parse_roots(args.roots)
    if not roots:
        policy_roots = (
            policy_engine.roots_for_agent(KNOWLEDGE_AGENT, include_manual=False)
            if policy_engine and policy_engine.has_policies
            else []
        )
        roots = policy_roots or [Path.cwd()]

    deduped_roots: List[Path] = []
    seen_roots: Set[str] = set()
    for root in roots:
        resolved = Path(root).expanduser()
        try:
            resolved = resolved.resolve()
        except OSError:
            pass
        key = str(resolved)
        if key in seen_roots:
            continue
        seen_roots.add(key)
        deduped_roots.append(resolved)

    roots = deduped_roots
    existing_roots: List[Path] = []
    for root in roots:
        if root.exists():
            existing_roots.append(root)
        else:
            print(f"âš ï¸ ê°ì‹œ ë£¨íŠ¸ê°€ ì¡´ì¬í•˜ì§€ ì•Šì•„ ì œì™¸í•©ë‹ˆë‹¤: {root}")
    if not existing_roots:
        existing_roots = [Path.cwd()]
    roots = existing_roots

    event_queue: "queue.Queue[Tuple[str, str]]" = queue.Queue()
    allowed_exts = {ext.lower() for ext in FileFinder.DEFAULT_EXTS}
    handler = WatchEventHandler(event_queue, allowed_exts, policy_engine=policy_engine, agent=KNOWLEDGE_AGENT)
    observer = Observer()
    for root in roots:
        observer.schedule(handler, str(root), recursive=True)

    pipeline_ctx = IncrementalPipeline(
        encoder=encoder,
        batch_size=batch_size,
        scan_csv=Path(args.scan_csv),
        corpus_path=Path(args.corpus),
        cache_dir=Path(args.cache),
        translate=args.translate,
        policy_engine=policy_engine,
    )

    debounce_sec = max(0.5, args.debounce_ms / 1000.0)
    stop_event = threading.Event()

    policy_info = " (ì •ì±… ê¸°ë°˜)" if policy_engine and policy_engine.has_policies else ""
    print(
        "ğŸ‘€ íŒŒì¼ ë³€ê²½ ê°ì‹œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. (Ctrl+Cë¡œ ì¢…ë£Œ)"
        f"\n   roots{policy_info}: {', '.join(str(r) for r in roots)}"
        f"\n   embedding model: {model_name} (batch={batch_size})"
        f"\n   debounce: {debounce_sec:.2f}s"
    )

    observer.start()
    try:
        _watch_loop(event_queue, pipeline_ctx, stop_event, debounce_sec)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ watcherë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    finally:
        stop_event.set()
        observer.stop()
        observer.join()
        try:
            _get_sentence_encoder_manager().release(model_name)
        except Exception:
            pass


def cmd_schedule(args):
    policy_engine = _load_policy_engine(getattr(args, "policy", None))
    if not policy_engine or not policy_engine.has_policies:
        print("âš ï¸ ìŠ¤ì¼€ì¤„ëŸ¬: ì •ì±…ì´ ì—†ì–´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    if args.agent != KNOWLEDGE_AGENT:
        print("âš ï¸ ìŠ¤ì¼€ì¤„ëŸ¬: í˜„ì¬ëŠ” knowledge_search ì—ì´ì „íŠ¸ ì˜ˆì•½ë§Œ ì§€ì›í•©ë‹ˆë‹¤.")
        return

    scheduler = JobScheduler()
    jobs = _register_policy_jobs(
        scheduler,
        policy_engine=policy_engine,
        agent=args.agent,
        output_root=Path(args.output_root),
        translate=args.translate,
    )

    if not jobs:
        print("âš ï¸ ìŠ¤ì¼€ì¤„ëŸ¬: ì˜ˆì•½ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤. ì •ì±…ì˜ indexing.modeë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    for job in jobs:
        next_run = job.next_run.isoformat() if job.next_run else "manual"
        print(f"â±ï¸ {job.metadata.get('path', job.name)} â†’ ë‹¤ìŒ ì‹¤í–‰: {next_run}")

    poll = max(5.0, float(getattr(args, "poll_seconds", 60.0)))
    if getattr(args, "once", False):
        scheduler.run_pending()
        return

    print("ğŸš€ ì •ì±… ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. (Ctrl+Cë¡œ ì¢…ë£Œ)")
    try:
        while True:
            scheduler.run_pending()
            time.sleep(poll)
    except KeyboardInterrupt:
        print("ğŸ‘‹ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")


def main():
    ap = argparse.ArgumentParser(prog="infopilot", description="InfoPilot CLI - ë‹¤êµ­ì–´ ë¬¸ì„œ ê²€ìƒ‰ê¸°")
    sp = ap.add_subparsers(dest="cmd", required=True)

    # scan
    ap_scan = sp.add_parser("scan", help="ë“œë¼ì´ë¸Œ ìŠ¤ìº”í•˜ì—¬ íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘")
    ap_scan.add_argument("--out", default=str(DEFAULT_FOUND_FILES))
    ap_scan.add_argument(
        "--root",
        "--roots",
        dest="roots",
        action="append",
        help="ìŠ¤ìº”í•  ë£¨íŠ¸ ë””ë ‰í„°ë¦¬. ì—¬ëŸ¬ ë²ˆ ì§€ì • ê°€ëŠ¥. ë¯¸ì§€ì • ì‹œ ì „ì²´ ìŠ¤ìº”.",
    )
    ap_scan.add_argument(
        "--policy",
        default=str(DEFAULT_POLICY_PATH),
        help="ìŠ¤ë§ˆíŠ¸ í´ë” ì •ì±… íŒŒì¼ ê²½ë¡œ (ë¹„í™œì„±í™”í•˜ë ¤ë©´ 'none').",
    )
    ap_scan.set_defaults(func=cmd_scan)

    # train
    ap_train = sp.add_parser(
        "train",
        help="ì½”í¼ìŠ¤ ìƒì„± + ëª¨ë¸ í•™ìŠµ (ê¸°ë³¸: ë²ˆì—­ ë¹„í™œì„±, ë‹¤êµ­ì–´ Sentence-BERT)",
    )
    ap_train.add_argument("--scan_csv", default=str(DEFAULT_FOUND_FILES))
    ap_train.add_argument("--corpus", default=str(CORPUS_PATH))
    ap_train.add_argument("--model", default=str(TOPIC_MODEL_PATH))
    ap_train.add_argument("--max_features", type=int, default=50000)
    ap_train.add_argument("--n_components", type=int, default=DEFAULT_N_COMPONENTS)
    ap_train.add_argument("--n_clusters", type=int, default=25)
    ap_train.add_argument("--min_df", type=int, default=2)
    ap_train.add_argument("--max_df", type=float, default=0.85)
    ap_train.add_argument("--embedding-model", default=DEFAULT_EMBED_MODEL, help="Sentence-BERT ì„ë² ë”© ëª¨ë¸ ì´ë¦„")
    ap_train.add_argument("--embedding-batch-size", type=int, default=32, help="Sentence-BERT ë°°ì¹˜ í¬ê¸°")
    ap_train.add_argument(
        "--limit",
        "--limit-files",
        dest="limit_files",
        type=int,
        default=0,
        help="í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ìƒìœ„ Nê°œ íŒŒì¼ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤ (0=ì „ì²´).",
    )
    translate_group = ap_train.add_mutually_exclusive_group()
    translate_group.add_argument(
        "--translate",
        dest="translate",
        action="store_true",
        help="deep-translatorë¡œ ì˜ì–´ ë²ˆì—­ì„ ê°•ì œ í™œì„±í™”í•©ë‹ˆë‹¤.",
    )
    translate_group.add_argument(
        "--no-translate",
        dest="translate",
        action="store_false",
        help="ë²ˆì—­ ê¸°ëŠ¥ì„ ë¹„í™œì„±í™”í•˜ê³  ì›ë¬¸ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.",
    )
    ap_train.add_argument("--no-embedding", dest="use_embedding", action="store_false", help="Sentence-BERT ëŒ€ì‹  TF-IDF ë°±ì—… ê²½ë¡œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    ap_train.add_argument(
        "--policy",
        default=str(DEFAULT_POLICY_PATH),
        help="ìŠ¤ë§ˆíŠ¸ í´ë” ì •ì±… íŒŒì¼ ê²½ë¡œ (ë¹„í™œì„±í™”í•˜ë ¤ë©´ 'none').",
    )
    ap_train.set_defaults(translate=False)
    ap_train.set_defaults(use_embedding=True)
    ap_train.set_defaults(func=cmd_train)

    # pipeline
    ap_pipe = sp.add_parser(
        "pipeline",
        help="ìŠ¤ìº” í›„ ë°”ë¡œ í•™ìŠµê¹Œì§€ ì§„í–‰ (ê¸°ë³¸: ë²ˆì—­ ë¹„í™œì„±)",
    )
    ap_pipe.add_argument("--out", default=str(DEFAULT_FOUND_FILES))
    ap_pipe.add_argument(
        "--root",
        "--roots",
        dest="roots",
        action="append",
        help="ìŠ¤ìº”í•  ë£¨íŠ¸ ë””ë ‰í„°ë¦¬. ì—¬ëŸ¬ ë²ˆ ì§€ì • ê°€ëŠ¥. ë¯¸ì§€ì • ì‹œ ì „ì²´ ìŠ¤ìº”.",
    )
    ap_pipe.add_argument("--corpus", default=str(CORPUS_PATH))
    ap_pipe.add_argument("--model", default=str(TOPIC_MODEL_PATH))
    ap_pipe.add_argument("--cache", default=str(CACHE_DIR))
    ap_pipe.add_argument("--max_features", type=int, default=50000)
    ap_pipe.add_argument("--n_components", type=int, default=DEFAULT_N_COMPONENTS)
    ap_pipe.add_argument("--n_clusters", type=int, default=25)
    ap_pipe.add_argument("--min_df", type=int, default=2)
    ap_pipe.add_argument("--max_df", type=float, default=0.85)
    ap_pipe.add_argument("--embedding-model", default=DEFAULT_EMBED_MODEL, help="Sentence-BERT ì„ë² ë”© ëª¨ë¸ ì´ë¦„")
    ap_pipe.add_argument("--embedding-batch-size", type=int, default=32, help="Sentence-BERT ë°°ì¹˜ í¬ê¸°")
    ap_pipe.add_argument(
        "--limit",
        "--limit-files",
        dest="limit_files",
        type=int,
        default=0,
        help="í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ìƒìœ„ Nê°œ íŒŒì¼ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤ (0=ì „ì²´).",
    )
    translate_group_pipe = ap_pipe.add_mutually_exclusive_group()
    translate_group_pipe.add_argument(
        "--translate",
        dest="translate",
        action="store_true",
        help="deep-translatorë¡œ ì˜ì–´ ë²ˆì—­ì„ ê°•ì œ í™œì„±í™”í•©ë‹ˆë‹¤.",
    )
    translate_group_pipe.add_argument(
        "--no-translate",
        dest="translate",
        action="store_false",
        help="ë²ˆì—­ ê¸°ëŠ¥ì„ ë¹„í™œì„±í™”í•˜ê³  ì›ë¬¸ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.",
    )
    ap_pipe.add_argument("--no-embedding", dest="use_embedding", action="store_false", help="Sentence-BERT ëŒ€ì‹  TF-IDF ë°±ì—… ê²½ë¡œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    ap_pipe.add_argument(
        "--launch-chat",
        action="store_true",
        help="íŒŒì´í”„ë¼ì¸ ì™„ë£Œ í›„ chat ëª¨ë“œë¥¼ ë°”ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.",
    )
    ap_pipe.add_argument(
        "--policy",
        default=str(DEFAULT_POLICY_PATH),
        help="ìŠ¤ë§ˆíŠ¸ í´ë” ì •ì±… íŒŒì¼ ê²½ë¡œ (ë¹„í™œì„±í™”í•˜ë ¤ë©´ 'none').",
    )
    ap_pipe.set_defaults(translate=False)
    ap_pipe.set_defaults(use_embedding=True)
    ap_pipe.set_defaults(func=cmd_pipeline)

    # chat
    ap_chat = sp.add_parser(
        "chat",
        help="ëŒ€í™”í˜• ì§ˆì˜ ëª¨ë“œ (ê¸°ë³¸: ë²ˆì—­ ë¹„í™œì„±, ë‹¤êµ­ì–´ Sentence-BERT)",
    )
    ap_chat.add_argument("--model", default=str(TOPIC_MODEL_PATH))
    ap_chat.add_argument("--corpus", default=str(CORPUS_PATH))
    ap_chat.add_argument("--cache", default=str(CACHE_DIR))
    ap_chat.add_argument("--scan_csv", default=str(DEFAULT_FOUND_FILES))
    ap_chat.add_argument("--topk", type=int, default=5)
    ap_chat.add_argument(
        "--scope",
        choices=["auto", "policy", "global"],
        default="auto",
        help="ê²€ìƒ‰ ë²”ìœ„: auto(ì •ì±… ìˆìœ¼ë©´ ì ìš©), policy(ì •ì±… ê°•ì œ), global(ì „ì²´)"
    )
    translate_group_chat = ap_chat.add_mutually_exclusive_group()
    translate_group_chat.add_argument(
        "--translate",
        dest="translate",
        action="store_true",
        help="deep-translatorë¡œ ì§ˆì˜ë¥¼ ì˜ì–´ ë²ˆì—­í•œ ë’¤ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
    )
    translate_group_chat.add_argument(
        "--no-translate",
        dest="translate",
        action="store_false",
        help="ì§ˆë¬¸ ë²ˆì—­ ê¸°ëŠ¥ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.",
    )
    ap_chat.add_argument("--no-auto-train", dest="auto_train", action="store_false", help="ìë™ í•™ìŠµ ê°±ì‹ ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
    rerank_group = ap_chat.add_mutually_exclusive_group()
    rerank_group.add_argument("--rerank", dest="rerank", action="store_true", help="Cross-Encoder ì¬ë­í‚¹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤ (ê¸°ë³¸ê°’).")
    rerank_group.add_argument("--no-rerank", dest="rerank", action="store_false", help="Cross-Encoder ì¬ë­í‚¹ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
    ap_chat.add_argument("--rerank-model", default="cross-encoder/ms-marco-MiniLM-L-6-v2", help="ì¬ë­í‚¹ì— ì‚¬ìš©í•  Cross-Encoder ëª¨ë¸ ì´ë¦„")
    ap_chat.add_argument("--rerank-depth", type=int, default=80, help="Cross-Encoder ì¬ë­í‚¹ì— í¬í•¨í•  í›„ë³´ ë¬¸ì„œ ìˆ˜ (50~100 ê¶Œì¥)")
    ap_chat.add_argument("--rerank-batch-size", type=int, default=16, help="Cross-Encoder ì¶”ë¡  ë°°ì¹˜ í¬ê¸° (CPU í™˜ê²½ì€ 8~16 ê¶Œì¥)")
    ap_chat.add_argument("--rerank-device", default=None, help="ì¬ë­í‚¹ ëª¨ë¸ì„ ë¡œë“œí•  ë””ë°”ì´ìŠ¤(e.g. 'cuda', 'cuda:0', 'cpu')")
    ap_chat.add_argument(
        "--rerank-min-score",
        type=float,
        default=0.35,
        help="Cross-Encoder ì ìˆ˜ê°€ ì´ ê°’ë³´ë‹¤ ë‚®ì€ ë¬¸ì„œëŠ” ì œì™¸í•©ë‹ˆë‹¤.",
    )
    ap_chat.add_argument(
        "--lexical-weight",
        type=float,
        default=0.0,
        help="BM25 ê°€ì¤‘ì¹˜ (0=ì˜ë¯¸ ê²€ìƒ‰ ì „ìš©). í•„ìš” ì‹œ ìˆ˜ë™ ì¡°ì •",
    )
    ap_chat.add_argument(
        "--min-similarity",
        type=float,
        default=0.35,
        help="ì´ ê°’ë³´ë‹¤ ë‚®ì€ ìœ ì‚¬ë„ ë¬¸ì„œëŠ” ì œì™¸í•©ë‹ˆë‹¤ (0.0~1.0).",
    )
    ap_chat.add_argument("--show-translation", action="store_true", help="ê²€ìƒ‰ ê²°ê³¼ì— ë²ˆì—­ë³¸ì„ í•¨ê»˜ í‘œì‹œí•©ë‹ˆë‹¤.")
    ap_chat.add_argument("--translation-lang", default="en", help="ë²ˆì—­ ëŒ€ìƒ ì–¸ì–´ ì½”ë“œ (ê¸°ë³¸: en)")
    ap_chat.add_argument(
        "--policy",
        default=str(DEFAULT_POLICY_PATH),
        help="ìŠ¤ë§ˆíŠ¸ í´ë” ì •ì±… íŒŒì¼ ê²½ë¡œ (ë¹„í™œì„±í™”í•˜ë ¤ë©´ 'none').",
    )
    ap_chat.add_argument(
        "--query",
        help="ë¹„ëŒ€í™”í˜• ëª¨ë“œì—ì„œ ë‹¨ì¼ ì§ˆì˜ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.",
    )
    ap_chat.add_argument(
        "--json",
        action="store_true",
        help="ì§ˆì˜ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì¶œë ¥í•˜ê³  ì¢…ë£Œí•©ë‹ˆë‹¤ (ë¹„ëŒ€í™”í˜• ëª¨ë“œ).",
    )
    ap_chat.set_defaults(translate=False)
    ap_chat.set_defaults(auto_train=True)
    ap_chat.set_defaults(rerank=True)
    ap_chat.set_defaults(func=cmd_chat)

    # watch
    ap_watch = sp.add_parser("watch", help="íŒŒì¼ ë³€ê²½ì„ ê°ì§€í•´ ì½”í¼ìŠ¤/ì¸ë±ìŠ¤ë¥¼ ì¦ë¶„ ê°±ì‹ ")
    ap_watch.add_argument("--root", "--roots", dest="roots", action="append", help="ê°ì‹œí•  ë£¨íŠ¸ ë””ë ‰í„°ë¦¬ (ì—¬ëŸ¬ ë²ˆ ì§€ì • ê°€ëŠ¥)")
    ap_watch.add_argument("--scan_csv", default=str(DEFAULT_FOUND_FILES))
    ap_watch.add_argument("--corpus", default=str(CORPUS_PATH))
    ap_watch.add_argument("--model", default=str(TOPIC_MODEL_PATH))
    ap_watch.add_argument("--cache", default=str(CACHE_DIR))
    ap_watch.add_argument("--debounce-ms", type=int, default=2000, help="íŒŒì¼ ì´ë²¤íŠ¸ ë””ë°”ìš´ìŠ¤ ì‹œê°„(ms)")
    ap_watch.add_argument("--translate", action="store_true", help="ì¦ë¶„ ì¶”ì¶œ ì‹œ ë²ˆì—­ì„ í¬í•¨í•©ë‹ˆë‹¤.")
    ap_watch.add_argument(
        "--policy",
        default=str(DEFAULT_POLICY_PATH),
        help="ìŠ¤ë§ˆíŠ¸ í´ë” ì •ì±… íŒŒì¼ ê²½ë¡œ (ë¹„í™œì„±í™”í•˜ë ¤ë©´ 'none').",
    )
    ap_watch.set_defaults(func=cmd_watch)

    # schedule
    ap_schedule = sp.add_parser("schedule", help="ì •ì±… ê¸°ë°˜ ì˜ˆì•½ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
    ap_schedule.add_argument(
        "--policy",
        default=str(DEFAULT_POLICY_PATH),
        help="ìŠ¤ë§ˆíŠ¸ í´ë” ì •ì±… íŒŒì¼ ê²½ë¡œ (ë¹„í™œì„±í™”í•˜ë ¤ë©´ 'none').",
    )
    ap_schedule.add_argument(
        "--agent",
        default=KNOWLEDGE_AGENT,
        choices=["knowledge_search", "meeting", "photo"],
        help="ì˜ˆì•½ ì‹¤í–‰ ëŒ€ìƒ ì—ì´ì „íŠ¸",
    )
    ap_schedule.add_argument(
        "--output-root",
        default=str(DEFAULT_SCHEDULED_ROOT),
        help="ì •ì±…ë³„ ì‚°ì¶œë¬¼ì„ ì €ì¥í•  ë£¨íŠ¸ ë””ë ‰í„°ë¦¬",
    )
    ap_schedule.add_argument(
        "--translate",
        action="store_true",
        help="ì˜ˆì•½ í•™ìŠµ ì‹œ ë²ˆì—­ íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.",
    )
    ap_schedule.add_argument(
        "--once",
        action="store_true",
        help="ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì‘ì—…ë§Œ ìˆ˜í–‰ í›„ ì¢…ë£Œí•©ë‹ˆë‹¤.",
    )
    ap_schedule.add_argument(
        "--poll-seconds",
        type=float,
        default=60.0,
        help="ì˜ˆì•½ ì‘ì—… í™•ì¸ ê°„ê²©(ì´ˆ). ìµœì†Œ 5ì´ˆ",
    )
    ap_schedule.set_defaults(translate=False)
    ap_schedule.set_defaults(func=cmd_schedule)

    args = ap.parse_args()
    args.func(args)


if __name__ == "__main__":
    main()
