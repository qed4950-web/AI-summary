import argparse
import sys
from pathlib import Path
import pandas as pd
import os
import string
from tqdm import tqdm

# Add project root to sys.path
sys.path.append(str(Path(__file__).parent.parent.parent))

# ì¤‘ì•™ ì„¤ì • ë° ìƒˆë¡œìš´ ì¿¼ë¦¬ íŒŒì„œ ê°€ì ¸ì˜¤ê¸°
from src.config import (
    EXCLUDE_DIRS, SUPPORTED_EXTS,
    FOUND_FILES_CSV, CORPUS_PARQUET, CACHE_DIR,
    DEFAULT_TOP_K
)
from src.core.corpus import CorpusBuilder
from src.core.indexing import run_indexing
from src.app.chat import LNPChat
from src.core.query_parser import parse_query_and_filters # ìƒˆë¡œìš´ ì¿¼ë¦¬ íŒŒì„œ import

def get_drives():
    """ì‹œìŠ¤í…œì— ì¡´ì¬í•˜ëŠ” ë“œë¼ì´ë¸Œ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    drives = []
    for letter in string.ascii_uppercase:
        drive = f"{letter}:\\"
        if os.path.exists(drive):
            drives.append(drive)
    return drives

def cmd_scan(args):
    """íŒŒì¼ ì‹œìŠ¤í…œì„ ìŠ¤ìº”í•˜ì—¬ íŒŒì¼ ëª©ë¡ê³¼ ë©”íƒ€ë°ì´í„°(í¬ê¸°, ìˆ˜ì •ì‹œê°„)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    file_rows = []
    drives = get_drives()
    print(f"ğŸ” Starting scan on drives: {', '.join(drives)}")
    print(f"ğŸš« Excluding directories containing: {', '.join(sorted(list(EXCLUDE_DIRS)))}")
    for drive in drives:
        print(f"Scanning drive {drive}...")
        try:
            for root, dirs, files in tqdm(os.walk(drive, topdown=True), desc=f"Scanning {drive}", encoding='utf-8'):
                dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]
                for file in files:
                    try:
                        p = Path(root) / file
                        if p.suffix.lower() in SUPPORTED_EXTS and not any(part in EXCLUDE_DIRS for part in p.parts):
                            stat = p.stat()
                            file_rows.append({
                                'path': str(p),
                                'size': stat.st_size,
                                'mtime': stat.st_mtime
                            })
                    except (FileNotFoundError, PermissionError): continue
        except PermissionError:
            print(f"Could not access {drive}. Skipping.")
            continue
    out = Path(args.out)
    out.parent.mkdir(parents=True, exist_ok=True)
    pd.DataFrame(file_rows).to_csv(out, index=False, encoding='utf-8')
    print(f"ğŸ“¦ ìŠ¤ìº” ê²°ê³¼ ì €ì¥: {out} ({len(file_rows)}ê°œ íŒŒì¼)")

def cmd_train(args):
    """(ì „ì²´ í•™ìŠµ) ìŠ¤ìº”ëœ íŒŒì¼ ëª©ë¡ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³ , ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    scan_csv_path = Path(args.scan_csv)
    corpus_path = Path(args.corpus)
    print(f"ğŸ“¥ ìŠ¤ìº” ëª©ë¡ ë¡œë“œ: {scan_csv_path}")
    file_rows = pd.read_csv(scan_csv_path).to_dict('records')
    print("ğŸ› ï¸ ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ìš”ì•½ ìƒì„± ì‹œì‘...")
    cb = CorpusBuilder(progress=True, max_workers=0)
    df_corpus = cb.build(file_rows)
    cb.save(df_corpus, corpus_path)
    print(f"ğŸ’¾ ì½”í¼ìŠ¤ ë° ì„±ê³µ/ì‹¤íŒ¨ ëª©ë¡ ì €ì¥ ì™„ë£Œ.")
    print("ğŸš€ Starting semantic indexing...")
    run_indexing(corpus_path=corpus_path, cache_dir=Path(args.cache))

def cmd_update(args):
    """(ì¦ë¶„ ì—…ë°ì´íŠ¸) ë³€ê²½ëœ íŒŒì¼ë§Œ ê°ì§€í•˜ì—¬ ì½”í¼ìŠ¤ì™€ ì¸ë±ìŠ¤ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    corpus_path = Path(args.corpus)
    if not corpus_path.exists():
        print("ê¸°ì¡´ ì½”í¼ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € 'train' ëª…ë ¹ìœ¼ë¡œ ì „ì²´ í•™ìŠµì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.", file=sys.stderr)
        return

    print("ğŸ“¥ ê¸°ì¡´ ì½”í¼ìŠ¤ ë¡œë”© ì¤‘...")
    df_old = pd.read_parquet(corpus_path)
    print(f"ê¸°ì¡´ ì½”í¼ìŠ¤ ë¡œë“œ ì™„ë£Œ. ({len(df_old)}ê°œ í•­ëª©)")

    print("ğŸ” í˜„ì¬ íŒŒì¼ ì‹œìŠ¤í…œ ìŠ¤ìº” ì¤‘...")
    # ê¸°ì¡´ ì½”í¼ìŠ¤ì— ìˆëŠ” í™•ì¥ìë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤ìº”
    exts_to_scan = df_old['ext'].unique()
    # ì„ì‹œ ìŠ¤ìº” íŒŒì¼ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì§ì ‘ ìŠ¤ìº” ê²°ê³¼ë¥¼ ë°›ìŒ
    current_files_list = []
    drives = get_drives()
    for drive in drives:
        for root, dirs, files in os.walk(drive, topdown=True):
            dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]
            for file in files:
                try:
                    p = Path(root) / file
                    if p.suffix.lower() in exts_to_scan and not any(part in EXCLUDE_DIRS for part in p.parts):
                        stat = p.stat()
                        current_files_list.append({'path': str(p), 'size': stat.st_size, 'mtime': stat.st_mtime})
                except (FileNotFoundError, PermissionError): continue
    df_current = pd.DataFrame(current_files_list)
    print(f"í˜„ì¬ íŒŒì¼ ìŠ¤ìº” ì™„ë£Œ. ({len(df_current)}ê°œ íŒŒì¼ ë°œê²¬)")

    print("ğŸ”„ ë³€ê²½ëœ íŒŒì¼ ë¶„ì„ ì¤‘...")
    df_old.rename(columns={'mtime': 'mtime_old'}, inplace=True)
    df_merged = pd.merge(df_current, df_old, on='path', how='outer', suffixes=('', '_old'), indicator=True)

    deleted_files = df_merged[df_merged['_merge'] == 'right_only']
    new_files = df_merged[df_merged['_merge'] == 'left_only']
    both_df = df_merged[df_merged['_merge'] == 'both']
    modified_files = both_df[both_df['mtime'] > both_df['mtime_old']]

    print(f"- ì‹ ê·œ íŒŒì¼: {len(new_files)}ê°œ")
    print(f"- ìˆ˜ì •ëœ íŒŒì¼: {len(modified_files)}ê°œ")
    print(f"- ì‚­ì œëœ íŒŒì¼: {len(deleted_files)}ê°œ")

    files_to_process_paths = pd.concat([new_files['path'], modified_files['path']]).unique().tolist()

    if not files_to_process_paths and len(deleted_files) == 0:
        print("ğŸ‰ ë³€ê²½ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  ë°ì´í„°ê°€ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤!")
        return

    if files_to_process_paths:
        print(f"ğŸ› ï¸ {len(files_to_process_paths)}ê°œ íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘...")
        rows_to_process = df_current[df_current['path'].isin(files_to_process_paths)].to_dict('records')
        cb = CorpusBuilder(progress=True)
        df_new_corpus = cb.build(rows_to_process)
        print("í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ.")
    else:
        df_new_corpus = pd.DataFrame()

    print("ğŸ’¾ ì½”í¼ìŠ¤ ì—…ë°ì´íŠ¸ ì¤‘...")
    paths_to_remove = set(deleted_files['path'].tolist() + modified_files['path'].tolist())
    df_updated = df_old[~df_old['path'].isin(paths_to_remove)]
    final_corpus = pd.concat([df_updated, df_new_corpus], ignore_index=True)
    CorpusBuilder.save(final_corpus, corpus_path)
    print("ì½”í¼ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")

    print("ğŸš€ ë²¡í„° ì¸ë±ìŠ¤ ì¬ìƒì„± ì¤‘...")
    run_indexing(corpus_path=corpus_path, cache_dir=Path(args.cache))
    print("âœ¨ ëª¨ë“  ì—…ë°ì´íŠ¸ ê³¼ì • ì™„ë£Œ!")

def cmd_chat(args):
    """ëŒ€í™”í˜• ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ëª¨ë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."""
    chat_session = LNPChat(corpus_path=Path(args.corpus), cache_dir=Path(args.cache), topk=args.topk)
    chat_session.ready(rebuild=False)
    print("\nğŸ’¬ InfoPilot Chat (ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰) ëª¨ë“œì…ë‹ˆë‹¤. (ì¢…ë£Œ: 'exit' ë˜ëŠ” 'ì¢…ë£Œ')")
    while True:
        try:
            query = input("ì§ˆë¬¸> ").strip()
        except (EOFError, KeyboardInterrupt): print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤."); break
        if not query: continue
        if query.lower() in {"exit", "quit", "ì¢…ë£Œ"}: print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤."); break
        cleaned_query, filters = parse_query_and_filters(query)
        print(f"[DEBUG] Cleaned Query: '{cleaned_query}', Filters: {filters}") # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
        result = chat_session.ask(cleaned_query, filters=filters)
        print(result["answer"])
        if result.get("suggestions"): 
            print("\nğŸ’¡ ì´ëŸ° ì§ˆë¬¸ì€ ì–´ë– ì„¸ìš”?")
            for s in result["suggestions"]: print(f"   - {s}")
        print("-" * 80)

def main():
    ap = argparse.ArgumentParser(prog="infopilot", description="InfoPilot CLI - ì˜ë¯¸ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ ì—”ì§„")
    sp = ap.add_subparsers(dest="cmd", required=True)

    # scan
    ap_scan = sp.add_parser("scan", help="ë“œë¼ì´ë¸Œë¥¼ ìŠ¤ìº”í•˜ì—¬ íŒŒì¼ ëª©ë¡ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    ap_scan.add_argument("--out", default=str(FOUND_FILES_CSV), help=f"ìŠ¤ìº” ê²°ê³¼ CSV íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: {FOUND_FILES_CSV})")
    ap_scan.set_defaults(func=cmd_scan)

    # train (ì „ì²´ í•™ìŠµ)
    ap_train = sp.add_parser("train", help="(ì „ì²´ í•™ìŠµ) ìŠ¤ìº”ëœ íŒŒì¼ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
    ap_train.add_argument("--scan_csv", default=str(FOUND_FILES_CSV), help=f"ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ìŠ¤ìº” ê²°ê³¼ CSV (ê¸°ë³¸ê°’: {FOUND_FILES_CSV})")
    ap_train.add_argument("--corpus", default=str(CORPUS_PARQUET), help=f"ìƒì„±ë  ì½”í¼ìŠ¤ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: {CORPUS_PARQUET})")
    ap_train.add_argument("--cache", default=str(CACHE_DIR), help=f"ì¸ë±ìŠ¤ ìºì‹œ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: {CACHE_DIR})")
    ap_train.set_defaults(func=cmd_train)

    # update (ì¦ë¶„ ì—…ë°ì´íŠ¸)
    ap_update = sp.add_parser("update", help="(ì¦ë¶„ ì—…ë°ì´íŠ¸) ë³€ê²½ëœ íŒŒì¼ë§Œ ê°ì§€í•˜ì—¬ ì½”í¼ìŠ¤ì™€ ì¸ë±ìŠ¤ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.")
    ap_update.add_argument("--corpus", default=str(CORPUS_PARQUET), help=f"ì—…ë°ì´íŠ¸í•  ì½”í¼ìŠ¤ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: {CORPUS_PARQUET})")
    ap_update.add_argument("--cache", default=str(CACHE_DIR), help=f"ì¸ë±ìŠ¤ ìºì‹œ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: {CACHE_DIR})")
    ap_update.set_defaults(func=cmd_update)

    # chat
    ap_chat = sp.add_parser("chat", help="ëŒ€í™”í˜• ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    ap_chat.add_argument("--corpus", default=str(CORPUS_PARQUET), help=f"ì½”í¼ìŠ¤ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: {CORPUS_PARQUET})")
    ap_chat.add_argument("--cache", default=str(CACHE_DIR), help=f"ì¸ë±ìŠ¤ ìºì‹œ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: {CACHE_DIR})")
    ap_chat.add_argument("--topk", type=int, default=DEFAULT_TOP_K, help=f"ìƒìœ„ Kê°œ ê²°ê³¼ ë°˜í™˜ (ê¸°ë³¸ê°’: {DEFAULT_TOP_K})")
    ap_chat.set_defaults(func=cmd_chat)

    args = ap.parse_args()
    args.func(args)

if __name__ == "__main__":
    main()
